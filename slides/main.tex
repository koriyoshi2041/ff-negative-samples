% ============================================================================
% Forward-Forward Algorithm Research Presentation
% Comprehensive version with figures and code
% ============================================================================

\documentclass[11pt,aspectratio=169]{beamer}

% ============================================================================
% Packages
% ============================================================================
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{fancyvrb}
\usetikzlibrary{arrows.meta,positioning,shapes.geometric,calc}

% ============================================================================
% Theme Configuration
% ============================================================================
\usetheme{default}
\usecolortheme{seagull}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Frame numbering
\setbeamertemplate{footline}[frame number]

% ============================================================================
% Colors
% ============================================================================
\definecolor{BlackGray}{HTML}{333333}
\definecolor{DarkGray}{HTML}{666666}
\definecolor{LightGray}{HTML}{F0F0F0}
\definecolor{AlertRed}{HTML}{B22222}
\definecolor{SuccessGreen}{HTML}{228B22}
\definecolor{AccentBlue}{HTML}{1E3A5F}
\definecolor{CodeBg}{HTML}{F5F5F5}

% Apply colors
\setbeamercolor{normal text}{fg=BlackGray}
\setbeamercolor{frametitle}{fg=BlackGray}
\setbeamercolor{title}{fg=BlackGray}
\setbeamercolor{structure}{fg=DarkGray}
\setbeamercolor{itemize item}{fg=DarkGray}
\setbeamercolor{itemize subitem}{fg=DarkGray}
\setbeamercolor{block title}{fg=white,bg=AccentBlue}
\setbeamercolor{block body}{bg=LightGray}
\setbeamercolor{alerted text}{fg=AlertRed}

% ============================================================================
% Fonts
% ============================================================================
\usefonttheme{professionalfonts}
\setbeamerfont{title}{size=\LARGE,series=\bfseries}
\setbeamerfont{frametitle}{size=\large,series=\bfseries}
\setbeamerfont{framesubtitle}{size=\normalsize}

% ============================================================================
% Custom Commands
% ============================================================================
\newcommand{\al}[1]{\textcolor{AlertRed}{#1}}
\newcommand{\alg}[1]{\textcolor{SuccessGreen}{#1}}
\newcommand{\alb}[1]{\textcolor{AccentBlue}{#1}}
\newcommand{\hl}[1]{\textbf{#1}}
\newcommand{\code}[1]{\colorbox{CodeBg}{\texttt{\small #1}}}

% Figure path
\graphicspath{{../figures/}}

% ============================================================================
% Title Information
% ============================================================================
\title{Why Forward-Forward Hasn't Become\\the New Paradigm}
\subtitle{A Systematic Investigation into Transfer Learning Failures\\and Bio-Inspired Alternatives}
\author{Research Team}
\date{February 2026}
\institute{FF Algorithm Research}

% ============================================================================
% Document
% ============================================================================
\begin{document}

% ----------------------------------------------------------------------------
% Title Slide
% ----------------------------------------------------------------------------
\begin{frame}[plain]
    \titlepage
\end{frame}

% ----------------------------------------------------------------------------
% Hero Figure
% ----------------------------------------------------------------------------
\begin{frame}{Our Key Finding}
    \begin{center}
        \includegraphics[width=0.95\textwidth]{transfer_hero.png}
    \end{center}

    \vspace{-0.5em}
    \centering
    \small \textit{CwC-FF achieves 89\% transfer accuracy---the only biologically plausible method that outperforms random initialization.}
\end{frame}

% ----------------------------------------------------------------------------
% Outline
% ----------------------------------------------------------------------------
\begin{frame}{Outline}
    \tableofcontents
\end{frame}

% ============================================================================
\section{The Problem}
% ============================================================================

% ----------------------------------------------------------------------------
% Three Barriers
% ----------------------------------------------------------------------------
\begin{frame}{The Three Barriers to FF Adoption}
    \begin{center}
        \includegraphics[width=0.95\textwidth]{three_barriers.png}
    \end{center}
\end{frame}

% ----------------------------------------------------------------------------
% Why Biological Plausibility Matters
% ----------------------------------------------------------------------------
\begin{frame}{The Backpropagation Dilemma}

    \textbf{Backpropagation works, but it cannot exist in biological brains:}

    \vspace{0.5em}

    \begin{columns}[T]
        \begin{column}{0.55\textwidth}
            \begin{enumerate}
                \item \hl{Weight Transport Problem}
                \begin{itemize}
                    \item BP needs symmetric forward/backward weights
                    \item Real neurons don't have this
                \end{itemize}

                \vspace{0.3em}

                \item \hl{Global Error Signals}
                \begin{itemize}
                    \item Errors propagate through entire network
                    \item Neurons only have local information
                \end{itemize}

                \vspace{0.3em}

                \item \hl{Two-Phase Operation}
                \begin{itemize}
                    \item Forward pass $\rightarrow$ store $\rightarrow$ backward pass
                    \item Real neurons learn continuously
                \end{itemize}
            \end{enumerate}
        \end{column}

        \begin{column}{0.42\textwidth}
            \begin{block}{Hinton's Solution (2022)}
                \small
                \textit{``Replace the forward and backward passes with two forward passes: one with positive (real) data, one with negative (fake) data.''}

                \vspace{0.3em}

                $\rightarrow$ \alg{No backward pass needed}
            \end{block}
        \end{column}
    \end{columns}

\end{frame}

% ============================================================================
\section{The Forward-Forward Algorithm}
% ============================================================================

% ----------------------------------------------------------------------------
% How FF Works
% ----------------------------------------------------------------------------
\begin{frame}{The Forward-Forward Algorithm}

    \begin{columns}[T]
        \begin{column}{0.48\textwidth}
            \textbf{Core Mechanism}

            \vspace{0.5em}

            Two forward passes with different goals:

            \begin{itemize}
                \item \alg{Positive pass}: Real data + correct label
                \item \al{Negative pass}: Real data + wrong label
            \end{itemize}

            \vspace{0.5em}

            \textbf{Goodness Function}
            \begin{equation*}
                G(h) = \text{mean}(h^2)
            \end{equation*}

            \vspace{0.5em}

            \textbf{Training Objective}
            \begin{equation*}
                \mathcal{L} = \log(1 + e^{-(G_{pos} - \theta)}) + \log(1 + e^{(G_{neg} - \theta)})
            \end{equation*}
        \end{column}

        \begin{column}{0.50\textwidth}
            \textbf{Label Embedding (Critical Design)}

            \vspace{0.3em}

            \begin{block}{\small \texttt{overlay\_label(x, y)}}
                \small
                \texttt{x[:, :10] = 0} \hfill \textit{\# Clear first 10 pixels}\\
                \texttt{x[range(len(y)), y] = x.max()}\\
                \hfill \textit{\# Set pixel y to max}
            \end{block}

            \vspace{0.3em}

            \textbf{Example: digit ``3''}\\
            \small Input: \texttt{[0, 0, 0, \al{MAX}, 0, 0, 0, 0, 0, 0, image...]}

            \vspace{0.5em}

            \al{$\rightarrow$ This design creates non-transferable features!}
        \end{column}
    \end{columns}

\end{frame}

% ----------------------------------------------------------------------------
% Research Questions
% ----------------------------------------------------------------------------
\begin{frame}{Research Questions}

    \begin{table}
        \centering
        \begin{tabular}{clp{6cm}}
            \hline\hline
            \textbf{RQ} & \textbf{Question} & \textbf{Why It Matters} \\
            \hline
            \textbf{RQ1} & Which negative sampling strategy works best? & Core to FF's design---no systematic comparison exists \\
            \textbf{RQ2} & Can FF features transfer across tasks? & Critical for practical applications \\
            \textbf{RQ3} & Why does standard FF transfer poorly? & Understanding enables improvement \\
            \textbf{RQ4} & Can bio-inspired variants improve FF? & Bridges ML and neuroscience \\
            \hline\hline
        \end{tabular}
    \end{table}

    \vspace{1em}

    \begin{block}{Preview}
        Standard FF achieves \alg{94.5\%} on MNIST but features transfer \al{worse than random initialization}!
    \end{block}

\end{frame}

% ----------------------------------------------------------------------------
% Our Contributions
% ----------------------------------------------------------------------------
\begin{frame}{Our Contributions}

    \begin{enumerate}
        \item \textbf{First systematic comparison of 6 negative sampling strategies}
        \begin{itemize}
            \item Fair comparison: 1000 epochs per layer for each strategy
            \item Finding: Simple \texttt{wrong\_label} beats all complex alternatives
        \end{itemize}

        \vspace{0.5em}

        \item \textbf{Discovery of the ``Transfer Learning Paradox''}
        \begin{itemize}
            \item FF features transfer \al{worse} than random initialization
            \item First to systematically document this critical limitation
        \end{itemize}

        \vspace{0.5em}

        \item \textbf{Root cause analysis: Label embedding}
        \begin{itemize}
            \item Identified why FF fails: features coupled to source labels
            \item Provided evidence: activation variance, gradient analysis
        \end{itemize}

        \vspace{0.5em}

        \item \textbf{Implementation of 5 bio-inspired variants}
        \begin{itemize}
            \item Three-Factor Hebbian, Prospective FF, PCL-FF, Layer Collab
            \item Comprehensive failure analysis with code
        \end{itemize}

        \vspace{0.5em}

        \item \textbf{Validation of CwC-FF as the solution}
        \begin{itemize}
            \item \alg{89.05\%} transfer accuracy (vs 54.19\% standard FF)
        \end{itemize}
    \end{enumerate}

\end{frame}

% ============================================================================
\section{Experiments \& Results}
% ============================================================================

% ----------------------------------------------------------------------------
% RQ1: Negative Sampling
% ----------------------------------------------------------------------------
\begin{frame}{RQ1: Negative Sampling Strategy Comparison}
    \begin{center}
        \includegraphics[width=0.85\textwidth]{negative_strategy_fair.png}
    \end{center}

    \vspace{-0.5em}

    \small
    \textbf{Conclusion}: Simple is better. Hinton's original \texttt{wrong\_label} strategy (94.5\%) beats all complex alternatives.
\end{frame}

% ----------------------------------------------------------------------------
% Negative Sampling Strategies
% ----------------------------------------------------------------------------
\begin{frame}{RQ1: The 6 Strategies We Tested}
    \begin{columns}[T]
        \begin{column}{0.55\textwidth}
            \textbf{Negative Sampling Strategies:}

            \vspace{0.3em}

            \begin{enumerate}
                \item \texttt{wrong\_label}: x + random wrong label \textit{(Hinton)}
                \item \texttt{class\_confusion}: different image + same label
                \item \texttt{same\_class\_diff\_img}: different image + wrong label
                \item \texttt{hybrid\_mix}: $\alpha \cdot x_1 + (1-\alpha) \cdot x_2$ + wrong
                \item \texttt{noise\_augmented}: x + gaussian noise + wrong
                \item \texttt{masked}: x with random masking + wrong
            \end{enumerate}
        \end{column}

        \begin{column}{0.43\textwidth}
            \begin{table}
                \small
                \begin{tabular}{lc}
                    \hline\hline
                    \textbf{Strategy} & \textbf{Acc.} \\
                    \hline
                    \alg{wrong\_label} & \alg{94.50\%} \\
                    class\_confusion & 92.15\% \\
                    same\_class\_diff & 92.06\% \\
                    hybrid\_mix & 63.37\% \\
                    noise\_augmented & 46.10\% \\
                    masked & 30.97\% \\
                    \hline\hline
                \end{tabular}
            \end{table}

            \vspace{0.5em}

            \small
            \textit{1000 epochs per layer, fair comparison}
        \end{column}
    \end{columns}
\end{frame}

% ----------------------------------------------------------------------------
% RQ2: Transfer Learning Paradox
% ----------------------------------------------------------------------------
\begin{frame}{RQ2: The Transfer Learning Paradox}
    \begin{center}
        \includegraphics[width=0.85\textwidth]{insight_transfer_paradox.png}
    \end{center}
\end{frame}

% ----------------------------------------------------------------------------
% Transfer Results Table
% ----------------------------------------------------------------------------
\begin{frame}{RQ2: Transfer Learning Results}

    \textbf{Protocol}: Train on MNIST $\rightarrow$ Freeze features $\rightarrow$ Test on Fashion-MNIST

    \vspace{0.5em}

    \begin{table}
        \centering
        \begin{tabular}{lccc}
            \hline\hline
            \textbf{Method} & \textbf{Source (MNIST)} & \textbf{Transfer} & \textbf{vs Random} \\
            \hline
            \alg{CwC-FF} & \alg{98.71\%} & \alg{89.05\%} & \alg{+17.2\%} \\
            Backprop & 95.08\% & 75.49\% & +3.6\% \\
            Random Init & --- & 71.89\% & baseline \\
            \al{Standard FF} & 89.90\% & \al{54.19\%} & \al{-17.7\%} \\
            \hline\hline
        \end{tabular}
    \end{table}

    \vspace{0.8em}

    \begin{alertblock}{The Paradox}
        \centering
        Standard FF pretrained features are \textbf{worse than random initialization!}
    \end{alertblock}

\end{frame}

% ----------------------------------------------------------------------------
% t-SNE Visualization
% ----------------------------------------------------------------------------
\begin{frame}{RQ2: Feature Visualization (t-SNE)}
    \begin{center}
        \includegraphics[width=0.95\textwidth]{tsne_comparison.png}
    \end{center}

    \vspace{-0.5em}

    \small
    \textit{FF features (left) show scattered clusters on Fashion-MNIST, while BP features (right) are better organized.}
\end{frame}

% ----------------------------------------------------------------------------
% RQ3: Root Cause
% ----------------------------------------------------------------------------
\begin{frame}{RQ3: The Root Cause --- Label Embedding}
    \begin{center}
        \includegraphics[width=0.95\textwidth]{label_embedding_explained.png}
    \end{center}
\end{frame}

% ----------------------------------------------------------------------------
% Root Cause Explanation
% ----------------------------------------------------------------------------
\begin{frame}{RQ3: Why Label Embedding Breaks Transfer}

    \begin{columns}[T]
        \begin{column}{0.55\textwidth}
            \textbf{The Problem}

            \vspace{0.3em}

            \small
            MNIST training:\\
            \quad \texttt{pixel[3]} bright = digit ``3''\\
            \quad Network learns: ``pixel[3] + curves = positive''

            \vspace{0.3em}

            Fashion-MNIST transfer:\\
            \quad \texttt{pixel[3]} bright = ``Dress'' (not digit 3!)\\
            \quad But network still expects curves...

            \vspace{0.3em}

            \al{\textbf{Features are COUPLED to source labels!}}

            \vspace{0.5em}

            \textbf{Evidence}
            \begin{itemize}
                \item Label neurons: 10$\times$ higher activation variance
                \item First layer weights: label-detector patterns
                \item Gradient analysis: Label dimensions dominate
            \end{itemize}
        \end{column}

        \begin{column}{0.42\textwidth}
            \begin{block}{Standard FF Input}
                \centering
                $[\underbrace{l_0, ..., l_9}_{\text{label pixels}}, \underbrace{x_1, ..., x_n}_{\text{image}}]$

                \vspace{0.3em}

                \small
                Features = $f$(image, \al{LABEL})

                \vspace{0.3em}

                \al{Useless when labels change!}
            \end{block}

            \vspace{0.5em}

            \begin{block}{CwC-FF Input}
                \centering
                $[\underbrace{x_1, ..., x_n}_{\text{image only}}]$

                \vspace{0.3em}

                \small
                Features = $f$(image)

                \vspace{0.3em}

                \alg{Transfer beautifully!}
            \end{block}
        \end{column}
    \end{columns}

\end{frame}

% ============================================================================
\section{Bio-Inspired Extensions}
% ============================================================================

% ----------------------------------------------------------------------------
% Bio-Inspired Overview
% ----------------------------------------------------------------------------
\begin{frame}{RQ4: Bio-Inspired Variants Overview}
    \begin{center}
        \includegraphics[width=0.85\textwidth]{insight_bio_attempts.png}
    \end{center}

    \vspace{-0.5em}

    \small
    \textbf{Hypothesis}: More biologically realistic $\rightarrow$ better generalization? \al{Mostly no.}
\end{frame}

% ----------------------------------------------------------------------------
% Three-Factor Learning
% ----------------------------------------------------------------------------
\begin{frame}{Three-Factor Hebbian Learning}

    \textbf{Inspiration}: Neuromodulation (dopamine, acetylcholine, norepinephrine)

    \vspace{0.3em}

    \begin{columns}[T]
        \begin{column}{0.50\textwidth}
            \begin{equation*}
                \Delta W = f(\text{pre}) \times f(\text{post}) \times \underbrace{M(t)}_{\text{modulator}}
            \end{equation*}

            \vspace{0.5em}

            \textbf{Modulation types tested:}
            \begin{itemize}
                \item \texttt{top\_down}: higher $\rightarrow$ lower layers
                \item \texttt{reward\_pred}: RPE signal
                \item \texttt{layer\_agree}: correlation($L_i$, $L_{i+1}$)
            \end{itemize}
        \end{column}

        \begin{column}{0.48\textwidth}
            \begin{table}
                \small
                \begin{tabular}{lcc}
                    \hline\hline
                    \textbf{Type} & \textbf{Transfer} & \textbf{Result} \\
                    \hline
                    top\_down & 64.3\% & \alg{+1.5\%} \\
                    none & 62.8\% & baseline \\
                    layer\_agree & 59.8\% & \al{-3.0\%} \\
                    reward\_pred & 18.4\% & \al{FAILED} \\
                    \hline\hline
                \end{tabular}
            \end{table}

            \vspace{0.5em}

            \small
            \textbf{Verdict}: Marginal improvement at best.\\
            Modulation doesn't fix label coupling.
        \end{column}
    \end{columns}

\end{frame}

% ----------------------------------------------------------------------------
% Prospective FF
% ----------------------------------------------------------------------------
\begin{frame}{Prospective Configuration FF}

    \textbf{Inspiration}: [Song et al., Nature Neuroscience 2024] --- Anticipatory neural activity

    \vspace{0.5em}

    \begin{columns}[T]
        \begin{column}{0.48\textwidth}
            \textbf{Two-Phase Learning}
            \begin{enumerate}
                \item \textbf{Inference}: Infer target activity
                \item \textbf{Consolidation}: Update weights to match
            \end{enumerate}

            \vspace{0.5em}

            \textbf{The Problem:}\\
            \small
            Target inference uses label hints:\\
            \quad $h_{target} = h + \beta \cdot \text{feedback}(\text{label})$

            \vspace{0.3em}

            More iterations = \al{STRONGER label coupling}!
        \end{column}

        \begin{column}{0.48\textwidth}
            \begin{table}
                \small
                \begin{tabular}{ccc}
                    \hline\hline
                    \textbf{Iterations} & \textbf{MNIST} & \textbf{Transfer $\Delta$} \\
                    \hline
                    1 & 89.2\% & +5.3\% \\
                    10 & 85.1\% & +1.2\% \\
                    100 & 23.4\% & \al{-13.2\%} \\
                    \hline\hline
                \end{tabular}
            \end{table}

            \vspace{0.5em}

            \al{\textbf{Result}: FAILED}

            \vspace{0.3em}

            \small
            More iterations \textit{amplifies} label-specific features, making transfer worse.
        \end{column}
    \end{columns}

\end{frame}

% ----------------------------------------------------------------------------
% PCL-FF
% ----------------------------------------------------------------------------
\begin{frame}{Predictive Coding Light FF (PCL-FF)}

    \textbf{Inspiration}: [Nature Communications 2025] --- Predictive Coding in Cortical Circuits

    \vspace{0.5em}

    \begin{columns}[T]
        \begin{column}{0.52\textwidth}
            \textbf{The Killer Mechanism:}

            \vspace{0.3em}

            \small
            \texttt{sparsity\_penalty = h.abs().mean() * 0.1}

            \vspace{0.5em}

            Sparsity creates incentive for $h = 0$:
            \begin{itemize}
                \item negative pre-activation $\rightarrow$ 0 (ReLU)
                \item penalty pushes more toward 0
                \item cascade: more zeros $\rightarrow$ lower loss
                \item network learns ``dead = good''
            \end{itemize}
        \end{column}

        \begin{column}{0.45\textwidth}
            \begin{table}
                \small
                \begin{tabular}{lcc}
                    \hline\hline
                    \textbf{Metric} & \textbf{FF} & \textbf{PCL-FF} \\
                    \hline
                    MNIST & 90.0\% & \al{17.5\%} \\
                    Dead Neurons & 8\% & \al{100\%} \\
                    \hline\hline
                \end{tabular}
            \end{table}

            \vspace{0.5em}

            \al{\textbf{COMPLETE FAILURE}}

            \vspace{0.3em}

            \small
            The ``death cascade'':
            \begin{itemize}
                \item Epoch 50: 30\% alive
                \item Epoch 100: 10\% alive
                \item Epoch 500: 0\% alive
            \end{itemize}
        \end{column}
    \end{columns}

\end{frame}

% ----------------------------------------------------------------------------
% Layer Collaboration
% ----------------------------------------------------------------------------
\begin{frame}{Layer Collaboration}

    \textbf{Inspiration}: [Lorberbom et al., AAAI 2024] --- Inter-layer information flow

    \vspace{0.5em}

    \begin{columns}[T]
        \begin{column}{0.50\textwidth}
            \textbf{Idea}: Layers share goodness information

            \vspace{0.3em}

            \begin{equation*}
                G^l_{collab} = G^l + \gamma \cdot G^{l-1} + \gamma \cdot G^{l+1}
            \end{equation*}

            \vspace{0.5em}

            \textbf{$\gamma$ controls collaboration strength}
            \begin{itemize}
                \item $\gamma = 0$: Independent layers (baseline)
                \item $\gamma = 0.7$: Optimal collaboration
                \item $\gamma = 1.0$: Too much coupling
            \end{itemize}
        \end{column}

        \begin{column}{0.48\textwidth}
            \begin{center}
                \includegraphics[width=\textwidth]{layer_collab_heatmap.png}
            \end{center}

            \vspace{-0.5em}

            \small
            \textbf{Result}: +1.18\% on MNIST\\
            \al{Still doesn't fix transfer problem}
        \end{column}
    \end{columns}

\end{frame}

% ----------------------------------------------------------------------------
% Lessons from Failures
% ----------------------------------------------------------------------------
\begin{frame}{Lessons from Bio-Inspired Failures}
    \begin{center}
        \includegraphics[width=0.85\textwidth]{lessons_from_failures.png}
    \end{center}
\end{frame}

% ============================================================================
\section{The Solution: CwC-FF}
% ============================================================================

% ----------------------------------------------------------------------------
% CwC-FF Introduction
% ----------------------------------------------------------------------------
\begin{frame}{The Solution: Channel-wise Competitive FF (CwC-FF)}

    \textbf{Key Insight}: Remove the labels entirely from the input!

    \vspace{0.5em}

    \begin{columns}[T]
        \begin{column}{0.48\textwidth}
            \textbf{Standard FF}
            \begin{itemize}
                \item Input: $[\text{label}, \text{image}]$
                \item Positive: correct label
                \item Negative: wrong label
                \item \al{Features coupled to labels}
            \end{itemize}

            \vspace{0.5em}

            \small
            \texttt{x[:, :10] = 0}\\
            \texttt{x[:, label] = x.max()} \hfill \al{COUPLED!}
        \end{column}

        \begin{column}{0.48\textwidth}
            \textbf{CwC-FF}
            \begin{itemize}
                \item Input: $[\text{image}]$ only (no label!)
                \item Positive: high channel coherence
                \item Negative: low channel coherence
                \item \alg{Task-agnostic features}
            \end{itemize}

            \vspace{0.5em}

            \small
            Channels compete within layers:\\
            \quad Winners $\rightarrow$ positive signal\\
            \quad Losers $\rightarrow$ negative signal\\
            \alg{NO LABELS NEEDED!}
        \end{column}
    \end{columns}

\end{frame}

% ----------------------------------------------------------------------------
% CwC-FF Results
% ----------------------------------------------------------------------------
\begin{frame}{CwC-FF Results: The Only Method That Works}

    \begin{center}
        \includegraphics[width=0.75\textwidth]{radar_comparison.png}
    \end{center}

    \vspace{-0.5em}

    \begin{table}
        \centering
        \small
        \begin{tabular}{lcccc}
            \hline\hline
            \textbf{Method} & \textbf{MNIST} & \textbf{Transfer} & \textbf{vs Random} & \textbf{Bio-Plausible} \\
            \hline
            \alg{CwC-FF} & \alg{98.71\%} & \alg{89.05\%} & \alg{+17.2\%} & \alg{High} \\
            Backprop & 99.2\% & 75.49\% & +3.6\% & None \\
            Standard FF & 94.50\% & 54.19\% & -17.7\% & Very High \\
            \hline\hline
        \end{tabular}
    \end{table}

\end{frame}

% ============================================================================
\section{Discussion \& Conclusion}
% ============================================================================

% ----------------------------------------------------------------------------
% Summary Figure
% ----------------------------------------------------------------------------
\begin{frame}{Summary of Findings}
    \begin{center}
        \includegraphics[width=0.95\textwidth]{insight_summary.png}
    \end{center}
\end{frame}

% ----------------------------------------------------------------------------
% Key Insights
% ----------------------------------------------------------------------------
\begin{frame}{Key Insights}

    \begin{enumerate}
        \item \textbf{Simple negative sampling wins for training, but loses for transfer}
        \begin{itemize}
            \item Hinton's label embedding gives best source accuracy
            \item But creates a shortcut that destroys transferability
        \end{itemize}

        \vspace{0.5em}

        \item \textbf{Bio-inspired modifications don't help (mostly)}
        \begin{itemize}
            \item Three-factor, predictive coding, sparsity --- all real brain features
            \item None address the fundamental label embedding problem
        \end{itemize}

        \vspace{0.5em}

        \item \textbf{Label-free learning is key for transferable representations}
        \begin{itemize}
            \item CwC-FF removes labels from input entirely
            \item Forces network to learn actual visual features
        \end{itemize}

        \vspace{0.5em}

        \item \textbf{Trade-off: Source vs Transfer performance}
        \begin{itemize}
            \item Standard FF: 94.5\% source, 54\% transfer
            \item CwC-FF: 98.7\% source, 89\% transfer --- \alg{best of both!}
        \end{itemize}
    \end{enumerate}

\end{frame}

% ----------------------------------------------------------------------------
% Conclusion
% ----------------------------------------------------------------------------
\begin{frame}{Conclusion}

    \textbf{Why FF Hasn't Become the New Paradigm}

    \vspace{0.5em}

    \begin{columns}[T]
        \begin{column}{0.48\textwidth}
            \textbf{\al{Limitations}}
            \begin{itemize}
                \item 4.7\% accuracy gap to backprop
                \item \al{Catastrophic transfer failure}
                \item Label embedding creates shortcuts
                \item Bio-inspired fixes don't help
            \end{itemize}
        \end{column}

        \begin{column}{0.48\textwidth}
            \textbf{\alg{The Path Forward}}
            \begin{itemize}
                \item CwC-FF solves transfer issue
                \item Remove labels from input
                \item Channel competition works
                \item Potential for neuromorphic hardware
            \end{itemize}
        \end{column}
    \end{columns}

    \vspace{1em}

    \begin{block}{Take-Home Message}
        \centering
        Biological plausibility alone doesn't guarantee good ML properties.\\
        \hl{Understanding failure modes} leads to principled solutions.
    \end{block}

\end{frame}

% ----------------------------------------------------------------------------
% Future Work
% ----------------------------------------------------------------------------
\begin{frame}{Future Work}

    \begin{itemize}
        \item \textbf{Scale CwC-FF to larger datasets}
        \begin{itemize}
            \item CIFAR-10, ImageNet
            \item Combine with ASGE (ICASSP 2026) techniques
        \end{itemize}

        \vspace{0.5em}

        \item \textbf{Hybrid approaches}
        \begin{itemize}
            \item FF for early layers + BP for final layers
            \item Progressive training schemes
        \end{itemize}

        \vspace{0.5em}

        \item \textbf{Neuromorphic implementation}
        \begin{itemize}
            \item Intel Loihi, IBM TrueNorth
            \item Energy efficiency benefits
        \end{itemize}

        \vspace{0.5em}

        \item \textbf{Unsupervised / Self-supervised extensions}
        \begin{itemize}
            \item Remove supervision entirely
            \item Contrastive learning principles
        \end{itemize}
    \end{itemize}

\end{frame}

% ----------------------------------------------------------------------------
% References
% ----------------------------------------------------------------------------
\begin{frame}{References}

    \small
    \begin{itemize}
        \item Hinton, G. (2022). \textit{The Forward-Forward Algorithm: Some Preliminary Investigations}. arXiv:2212.13345

        \vspace{0.3em}

        \item Brenig, L., et al. (2023). \textit{Transfer Learning with FF}. First to show FF has transfer problems.

        \vspace{0.3em}

        \item Lorberbom, G., et al. (2024). \textit{Layer Collaboration}. AAAI 2024.

        \vspace{0.3em}

        \item Papachristodoulou, A., et al. (2024). \textit{CwC-FF: Channel-wise Competitive FF}.

        \vspace{0.3em}

        \item Song, Y., et al. (2024). \textit{Prospective Configuration}. Nature Neuroscience.

        \vspace{0.3em}

        \item Whittington, J. C. R., \& Bogacz, R. (2017). \textit{Predictive Coding Networks}. Neural Computation.
    \end{itemize}

\end{frame}

% ----------------------------------------------------------------------------
% Thank You
% ----------------------------------------------------------------------------
\begin{frame}[plain]
    \centering
    \vspace{3em}

    {\LARGE \textbf{Thank You}}

    \vspace{2em}

    {\large Questions?}

    \vspace{3em}

    \small
    Code and experiments available at:\\
    \texttt{github.com/koriyoshi2041/ff-negative-samples}

    \vspace{1em}

    \tiny
    Tested on Apple M4 Air | PyTorch 2.0+

\end{frame}

\end{document}

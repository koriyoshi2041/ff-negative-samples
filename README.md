# Forward-Forward Algorithm: Negative Sample Strategies & Transfer Learning

> Systematic comparison of negative sample strategies and investigation of transfer learning in Hinton's Forward-Forward algorithm.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ¯ Research Goals

1. **Negative Sample Strategy Comparison**: First systematic comparison of 10+ negative sample generation strategies for FF algorithm
2. **Transfer Learning Analysis**: Investigate why FF fails at transfer learning and potential solutions (Layer Collaboration, etc.)

## ğŸ“– Research Significance

### Why Systematic Negative Sample Comparison Matters

The Forward-Forward algorithm's performance is fundamentally tied to the quality of negative samples â€” yet **no prior work has systematically compared different strategies**. Hinton's original paper proposed label embedding, but subsequent works introduced mixing, masking, self-contrastive, and even negative-free variants (MonoForward, CwComp) without head-to-head comparison. This gap leaves practitioners without guidance on which strategy to use and researchers without understanding of *why* certain approaches work better. Our systematic comparison across 10 strategies aims to: (1) establish empirical baselines, (2) identify which properties make negatives effective (hardness? diversity? distribution gap?), and (3) inform the design of next-generation strategies.

### Why Layer Collaboration + Transfer Learning is a Critical Gap

Brenig et al. (2023) demonstrated that FF's transfer learning performance "significantly lags behind BP in ALL studied settings" â€” with gaps up to 38.9%. They attributed this to FF's layer-wise loss functions discarding information unnecessary for the current task. The Layer Collaboration mechanism (AAAI 2024) was proposed to address FF's layer isolation problem by introducing global goodness signals. **However, the Layer Collaboration paper did not test transfer learning.** This is a critical gap: if layer collaboration improves information flow between layers, it *should* help preserve transferable features â€” but this hypothesis remains untested. Our work fills this gap by directly measuring whether Layer Collaboration improves FF's transfer learning, potentially identifying a path to make FF practically useful for real-world scenarios where pre-training and fine-tuning are essential.

---

## ğŸ”¬ Experiment Results

### ğŸš¨ Transfer Learning: FF vs BP (MNIST â†’ Fashion-MNIST)

> **æ ¸å¿ƒå‘ç°ï¼šFF è¿ç§»å­¦ä¹ æ€§èƒ½è¿œè½åäº BPï¼Œç”šè‡³ä¸å¦‚éšæœºåˆå§‹åŒ–ï¼**

![Transfer Learning Comparison](results/strategy_comparison.png)

| Method | Source Accuracy | Transfer Accuracy | Gap vs BP |
|--------|-----------------|-------------------|-----------|
| **BP (Backprop)** | 97.73% | **73.19%** | â€” |
| **Random Init** | â€” | **80.60%** | +7.41% |
| **FF Original** | 56.75% | **13.47%** | **-59.72%** âš ï¸ |
| **FF + Layer Collab (All)** | 48.12% | 10.00% | -63.19% |
| **FF + Layer Collab (Prev)** | 56.50% | 10.21% | -62.98% |

**ğŸ”¥ Key Insight**: 
- **Random initialization outperforms all FF variants!** (80.6% vs 13.47%)
- FF çš„é¢„è®­ç»ƒç‰¹å¾ä¸ä»…æ²¡æœ‰å¸®åŠ©ï¼Œåè€Œæœ‰å®³
- Layer Collaboration å¹¶æœªæ”¹å–„è¿ç§»å­¦ä¹ ï¼ˆå¯èƒ½éœ€è¦æ›´å¤šè°ƒä¼˜ï¼‰

---

### ğŸ“Š CKA Representation Analysis

**FF vs BP Layer Similarity**
![CKA Heatmap](results/visualizations/cka_ff_vs_bp.png)
> *è§£è¯»ï¼šå¯¹è§’çº¿å€¼ä» 0.444 é€’å‡åˆ° 0.038ï¼Œè¡¨æ˜å±‚æ•°è¶Šæ·±ï¼ŒFF ä¸ BP çš„å·®å¼‚è¶Šå¤§*

**Self-CKA Comparison: FF layers are disconnected**
![Self-CKA](results/visualizations/cka_self_comparison.png)
> *è§£è¯»ï¼šFF çƒ­åŠ›å›¾å‘ˆ"å—çŠ¶"ï¼ˆå±‚é—´ç‹¬ç«‹ï¼‰ï¼ŒBP å‘ˆ"å¹³æ»‘è¿‡æ¸¡"ï¼ˆå±‚é—´åä½œï¼‰*

| Metric | Value | Implication |
|--------|-------|-------------|
| FF vs BP Layer 0 CKA | 0.444 | âœ… Early layers similar |
| FF vs BP Layer 2 CKA | **0.038** | âš ï¸ High layers completely different |
| **FF L0â†”L2 Self-CKA** | **0.025** | ğŸš¨ **Catastrophic layer disconnection** |
| FF Self-CKA (avg) | 0.264 | Layer disconnection |
| BP Self-CKA (avg) | 0.592 | Information flows well |

**ğŸ”¥ Core Insight**: FF's transfer failure is caused by **catastrophic layer disconnection**:
- FF Layer 0 â†” Layer 2 CKA = **0.025** (almost completely independent!)
- BP's minimum cross-layer CKA = 0.36 (14Ã— higher)
- High layers learn features with no connection to early layers, making transfer impossible

---

### ğŸ“ˆ Negative Sample Strategy Comparison (Partial)

| Rank | Strategy | Accuracy | Time (s) | Uses Labels |
|------|----------|----------|----------|-------------|
| ğŸ¥‡ | **label_embedding** | 38.81% | 150.2 | âœ“ |
| ğŸ¥‡ | **class_confusion** | 38.81% | 105.5 | âœ“ |
| 3 | random_noise | 9.80% | 99.4 | âœ— |
| 3 | image_mixing | 9.80% | 100.6 | âœ— |

**âš ï¸ Note**: Non-label strategies show ~10% (random chance) because evaluation requires label embedding. Need linear probe for fair comparison.

*Status: 4/10 strategies completed. In progress: self_contrastive, masking, layer_wise, adversarial, hard_mining, mono_forward*

---

## ğŸ“Š Key Findings

### Why FF Fails at Transfer Learning (Brenig et al., 2023)
- FF matches BP on source task training, but **transfer performance significantly lags behind** (up to 38.9% gap)
- Root cause: Layer-wise loss functions discard information "unnecessary" for current task
- **Insight**: The problem isn't FF itself, but the training objective design

### Layer Collaboration (AAAI 2024)
- Adds global goodness term (Î³) to local optimization
- Improves MNIST error from 3.3% â†’ 2.1%
- **Our Finding**: Does NOT help transfer learning (needs further investigation)

### State-of-the-Art (2024-2025)
| Method | Source | Innovation | CIFAR-10 |
|--------|--------|------------|----------|
| CwComp | AAAI 2024 | No negative samples needed | 78% |
| SCFF | Nature 2025 | Self-contrastive, no labels | ~70% |
| Distance-FF | arXiv 2024 | Metric learning | SOTA |

---

## ğŸ”§ Implemented Strategies

All 10 strategies with unified interface:

| # | Strategy | Labels Required | Description |
|---|----------|-----------------|-------------|
| 1 | LabelEmbedding | âœ“ | Hinton's original method |
| 2 | ImageMixing | âœ— | Pixel-wise mixing of images |
| 3 | RandomNoise | âœ— | Pure noise baseline |
| 4 | ClassConfusion | âœ“ | Correct image + wrong label |
| 5 | SelfContrastive | âœ— | Strong augmentation (SCFF) |
| 6 | Masking | âœ— | Random pixel masking |
| 7 | LayerWise | âœ— | Layer-adaptive generation |
| 8 | Adversarial | âœ— | Gradient-based perturbation |
| 9 | HardMining | âœ“ | Select hardest negatives |
| 10 | MonoForward | âœ“ | No negatives variant |

---

## ğŸ“ Project Structure

```
ff-research/
â”œâ”€â”€ negative_strategies/     # 10 strategy implementations
â”‚   â”œâ”€â”€ base.py             # Base class + registry
â”‚   â”œâ”€â”€ label_embedding.py
â”‚   â”œâ”€â”€ image_mixing.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ analysis/               # Representation analysis
â”‚   â”œâ”€â”€ cka_analysis.py     # CKA similarity
â”‚   â””â”€â”€ linear_probe.py     # Linear probing
â”œâ”€â”€ experiments/            # Experiment scripts
â”‚   â”œâ”€â”€ ff_baseline.py      # Verified FF baseline
â”‚   â””â”€â”€ transfer_learning.py # Transfer learning experiments
â”œâ”€â”€ results/                # Experiment outputs
â”‚   â”œâ”€â”€ visualizations/     # CKA heatmaps
â”‚   â”œâ”€â”€ transfer/           # Transfer learning results
â”‚   â”œâ”€â”€ strategy_comparison.json
â”‚   â””â”€â”€ cka_summary.json
â”œâ”€â”€ literature/             # Paper analyses
â”‚   â”œâ”€â”€ brenig2023_analysis.md
â”‚   â”œâ”€â”€ lorberbom2024_layer_collab.md
â”‚   â””â”€â”€ opensource_survey.md
â””â”€â”€ KEY_FINDINGS.md         # Summary of discoveries
```

---

## ğŸš€ Quick Start

```python
from negative_strategies import LabelEmbeddingStrategy, ImageMixingStrategy

# Unified interface
strategy = LabelEmbeddingStrategy(num_classes=10)
positive = strategy.create_positive(images, labels)
negative = strategy.generate(images, labels)
```

---

## ğŸ“ˆ Experiment Status

### âœ… Completed
- [x] Literature review complete
- [x] 10 negative strategies implemented
- [x] CKA/Linear Probe code ready
- [x] CKA representation analysis (FF vs BP)
- [x] Transfer learning experiment (MNIST â†’ Fashion-MNIST)
- [x] Layer Collaboration implementation & test

### ğŸ”„ In Progress
- [ ] MNIST strategy comparison (4/10 done)
- [ ] Self-contrastive strategy testing

### ğŸ“‹ Planned
- [ ] Linear probe evaluation for non-label strategies
- [ ] CIFAR-10 experiments
- [ ] Investigate why Layer Collab didn't help transfer

---

## ğŸ“š References

- Hinton, G. (2022). The Forward-Forward Algorithm. arXiv:2212.13345
- Brenig et al. (2023). A Study of Forward-Forward for Self-Supervised Learning. arXiv:2309.11955
- Lorberbom et al. (2024). Layer Collaboration in Forward-Forward. AAAI 2024
- Nature Communications (2025). Self-Contrastive Forward-Forward

## ğŸ“ License

MIT

---

*Active research project by [Shuaizhi Cheng](https://github.com/koriyoshi2041)*

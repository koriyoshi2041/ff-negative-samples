# Forward-Forward Algorithm Research

<div align="center">

**[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

</div>

---

<a name="english"></a>
## English Version

### Core Findings

#### 1. CwC-FF: Revolutionary Architecture Without Negative Samples

| Model | MNIST Accuracy | Negative Samples | Architecture |
|-------|----------------|------------------|--------------|
| Standard FF (MLP) | 93.15% | Required | Fully Connected |
| **CwC-FF (CNN)** | **98.75%** | **Not Required** | Channel Competition |

**CwC-FF eliminates negative samples entirely through channel competition, while improving accuracy by 5.6%.**

<details>
<summary>ğŸ“ˆ View CwC-FF Learning Curve</summary>

![CwC-FF Learning Curve](./results/cwc_ff_learning_curve.png)

</details>

#### 2. Catastrophic Layer Disconnection

FF's inter-layer information flow is nearly zero - the root cause of transfer learning failure.

| Metric | FF | BP | Gap |
|--------|----|----|-----|
| Layer 0 â†” Layer 2 CKA | **0.025** | 0.39 | 15.6Ã— |
| Avg Inter-layer Coherence | 0.264 | 0.592 | 2.2Ã— |

<details>
<summary>ğŸ”¥ View CKA Heatmap</summary>

![CKA Heatmap](./results/cka_heatmap.png)

</details>

#### 3. Counter-intuitive Transfer Learning Discovery

MNIST â†’ Fashion-MNIST transfer:

| Method | Source Acc | Transfer Acc | vs Random Init |
|--------|------------|--------------|----------------|
| Random Init | N/A | **83.81%** | Baseline |
| BP Pretrained | 98.34% | 77.06% | âˆ’6.75% |
| FF Pretrained | 89.79% | 61.06% | **âˆ’22.75%** ğŸ”´ |

**Conclusion: FF pretrained weights hurt transfer learning.** FF's label-embedding design creates features strongly tied to source task labels, making them poorly transferable.

<details>
<summary>ğŸ“Š View Transfer Comparison</summary>

![Transfer Comparison](./results/transfer_comparison.png)

</details>

---

### Implementations

#### Models (4 types)

| Model | File | Description | Status |
|-------|------|-------------|--------|
| **FF Baseline** | `models/ff_correct.py` | Corrected standard FF | âœ… 93.15% |
| **Layer Collab** | `models/layer_collab_ff.py` | Layer Collaboration (AAAI 2024) | âœ… |
| **PFF** | `models/pff.py` | Predictive FF, dual-circuit | âœ… |
| **CwC-FF** | `models/cwc_ff.py` | Channel-wise Competitive FF | âœ… 98.75% |

#### Negative Sample Strategies (10 types)

| Strategy | Requires Labels | Description |
|----------|-----------------|-------------|
| `label_embedding` | âœ“ | Hinton's original: embed label in pixels |
| `class_confusion` | âœ“ | Correct image + wrong label |
| `random_noise` | âœ— | Pure random noise |
| `image_mixing` | âœ— | Pixel-wise image mixing |
| `self_contrastive` | âœ— | SCFF: self-contrastive (Nature 2025) |
| `masking` | âœ— | Random/block/patch masking |
| `layer_wise` | âœ— | Layer-adaptive negative samples |
| `adversarial` | âœ— | FGSM/PGD adversarial perturbation |
| `hard_mining` | âœ“ | Hard negative mining |
| `mono_forward` | - | No-negative variant (VICReg) |

---

### Critical Bug Fixes

| Bug | Wrong | Correct | Impact |
|-----|-------|---------|--------|
| **Goodness calculation** | `sum(dim=1)` | `mean(dim=1)` | Severe |
| **Label embedding value** | Fixed `1.0` | `x.max()` | Severe |
| **Training mode** | mini-batch, simultaneous | full-batch, layer-by-layer greedy | Severe |
| **SCFF input processing** | addition `x + x` | concatenation `cat([x, x])` | Severe |

**Accuracy after fixes: 38% â†’ 93%**

---

### Quick Start

```bash
# Install
cd ff-research
python -m venv venv
source venv/bin/activate
pip install torch torchvision matplotlib seaborn

# Run baseline (93% accuracy)
python experiments/ff_baseline.py

# Run CwC-FF (98.75% accuracy, no negative samples)
python experiments/cwc_full_test.py
```

---

### Our Unique Contributions

1. **First to test Layer Collaboration for transfer learning** â†’ Proved ineffective
2. **First to quantify FF's "layer disconnection" with CKA** â†’ L0-L2 CKA = 0.025
3. **First to prove FF pretrained weights are "harmful"** â†’ 67% worse than random

---

<a name="ä¸­æ–‡"></a>
## ä¸­æ–‡ç‰ˆæœ¬

### æ ¸å¿ƒå‘ç°

#### 1. CwC-FF: æ— éœ€è´Ÿæ ·æœ¬çš„é©å‘½æ€§æ¶æ„

| æ¨¡å‹ | MNISTå‡†ç¡®ç‡ | è´Ÿæ ·æœ¬ | æ¶æ„ |
|------|------------|--------|------|
| æ ‡å‡†FF (MLP) | 93.15% | éœ€è¦ | å…¨è¿æ¥ |
| **CwC-FF (CNN)** | **98.75%** | **ä¸éœ€è¦** | é€šé“ç«äº‰ |

**CwC-FF é€šè¿‡é€šé“ç«äº‰æœºåˆ¶å®Œå…¨æ¶ˆé™¤è´Ÿæ ·æœ¬éœ€æ±‚ï¼ŒåŒæ—¶å‡†ç¡®ç‡æå‡5.6%ã€‚**

<details>
<summary>ğŸ“ˆ æŸ¥çœ‹ CwC-FF å­¦ä¹ æ›²çº¿</summary>

![CwC-FF Learning Curve](./results/cwc_ff_learning_curve.png)

</details>

#### 2. å±‚æ–­è¿ç°è±¡ (Catastrophic Layer Disconnection)

FFçš„å±‚é—´ä¿¡æ¯æµå‡ ä¹ä¸ºé›¶ï¼Œè¿™æ˜¯è¿ç§»å­¦ä¹ å¤±è´¥çš„æ ¹æœ¬åŸå› ã€‚

| åº¦é‡ | FF | BP | å·®è· |
|------|----|----|------|
| Layer 0 â†” Layer 2 CKA | **0.025** | 0.39 | 15.6Ã— |
| å¹³å‡å±‚é—´ä¸€è‡´æ€§ | 0.264 | 0.592 | 2.2Ã— |

<details>
<summary>ğŸ”¥ æŸ¥çœ‹ CKA çƒ­åŠ›å›¾</summary>

![CKA Heatmap](./results/cka_heatmap.png)

</details>

#### 3. è¿ç§»å­¦ä¹ çš„åç›´è§‰å‘ç°

MNIST â†’ Fashion-MNIST è¿ç§»å®éªŒï¼š

| æ–¹æ³• | æºä»»åŠ¡å‡†ç¡®ç‡ | è¿ç§»å‡†ç¡®ç‡ | ä¸éšæœºåˆå§‹åŒ–æ¯”è¾ƒ |
|------|-------------|-----------|------------------|
| éšæœºåˆå§‹åŒ– | N/A | **83.81%** | åŸºå‡† |
| BPé¢„è®­ç»ƒ | 98.34% | 77.06% | âˆ’6.75% |
| FFé¢„è®­ç»ƒ | 89.79% | 61.06% | **âˆ’22.75%** ğŸ”´ |

**ç»“è®ºï¼šFFé¢„è®­ç»ƒçš„æƒé‡å¯¹è¿ç§»æœ‰å®³ã€‚** FFçš„label-embeddingè®¾è®¡å¯¼è‡´ç‰¹å¾ä¸æºä»»åŠ¡æ ‡ç­¾å¼ºç»‘å®šï¼Œè¿ç§»æ€§å·®ã€‚

<details>
<summary>ğŸ“Š æŸ¥çœ‹è¿ç§»å­¦ä¹ å¯¹æ¯”</summary>

![Transfer Comparison](./results/transfer_comparison.png)

</details>

---

### å®ç°æ¸…å•

#### æ¨¡å‹æ¶æ„ (4ç§)

| æ¨¡å‹ | æ–‡ä»¶ | æè¿° | çŠ¶æ€ |
|------|------|------|------|
| **FF Baseline** | `models/ff_correct.py` | ä¿®æ­£åçš„æ ‡å‡†FF | âœ… 93.15% |
| **Layer Collab** | `models/layer_collab_ff.py` | å±‚é—´ååŒ (AAAI 2024) | âœ… |
| **PFF** | `models/pff.py` | é¢„æµ‹æ€§FFï¼ŒåŒå›è·¯æ¶æ„ | âœ… |
| **CwC-FF** | `models/cwc_ff.py` | é€šé“ç«äº‰FFï¼Œæ— éœ€è´Ÿæ ·æœ¬ | âœ… 98.75% |

#### è´Ÿæ ·æœ¬ç­–ç•¥ (10ç§)

| ç­–ç•¥ | éœ€è¦æ ‡ç­¾ | æè¿° |
|------|----------|------|
| `label_embedding` | âœ“ | HintonåŸç‰ˆï¼šæ ‡ç­¾åµŒå…¥åƒç´  |
| `class_confusion` | âœ“ | æ­£ç¡®å›¾åƒ+é”™è¯¯æ ‡ç­¾ |
| `random_noise` | âœ— | çº¯éšæœºå™ªå£° |
| `image_mixing` | âœ— | ä¸¤å›¾åƒç´ æ··åˆ |
| `self_contrastive` | âœ— | SCFFï¼šè‡ªå¯¹æ¯”å­¦ä¹  (Nature 2025) |
| `masking` | âœ— | éšæœº/å—/patché®ç½© |
| `layer_wise` | âœ— | å±‚è‡ªé€‚åº”è´Ÿæ ·æœ¬ |
| `adversarial` | âœ— | FGSM/PGDå¯¹æŠ—æ‰°åŠ¨ |
| `hard_mining` | âœ“ | å›°éš¾è´Ÿæ ·æœ¬æŒ–æ˜ |
| `mono_forward` | - | æ— è´Ÿæ ·æœ¬å˜ä½“ (VICReg) |

---

### å…³é”®Bugä¿®å¤

| é—®é¢˜ | é”™è¯¯å®ç° | æ­£ç¡®å®ç° | å½±å“ |
|------|---------|---------|------|
| **Goodnessè®¡ç®—** | `sum(dim=1)` | `mean(dim=1)` | ä¸¥é‡ |
| **æ ‡ç­¾åµŒå…¥å€¼** | å›ºå®š `1.0` | `x.max()` | ä¸¥é‡ |
| **è®­ç»ƒæ–¹å¼** | mini-batch, åŒæ—¶è®­ç»ƒ | full-batch, layer-by-layer greedy | ä¸¥é‡ |
| **SCFFè¾“å…¥å¤„ç†** | åŠ æ³• `x + x` | æ‹¼æ¥ `cat([x, x])` | ä¸¥é‡ |

**ä¿®å¤åå‡†ç¡®ç‡ï¼š38% â†’ 93%**

---

### å¿«é€Ÿå¼€å§‹

```bash
# å®‰è£…
cd ff-research
python -m venv venv
source venv/bin/activate
pip install torch torchvision matplotlib seaborn

# è¿è¡ŒåŸºçº¿å®éªŒ (93% å‡†ç¡®ç‡)
python experiments/ff_baseline.py

# è¿è¡ŒCwC-FF (98.75% å‡†ç¡®ç‡ï¼Œæ— éœ€è´Ÿæ ·æœ¬)
python experiments/cwc_full_test.py
```

---

### æˆ‘ä»¬çš„ç‹¬ç‰¹è´¡çŒ®

1. **é¦–æ¬¡æµ‹è¯•Layer Collaborationçš„è¿ç§»èƒ½åŠ›** â†’ è¯æ˜æ— æ•ˆ
2. **é¦–æ¬¡ç”¨CKAé‡åŒ–FFçš„"å±‚æ–­è¿"** â†’ L0-L2 CKA=0.025
3. **é¦–æ¬¡è¯æ˜FFé¢„è®­ç»ƒæƒé‡"æœ‰å®³"** â†’ æ¯”éšæœºå·®67%

---

### æ ¸å¿ƒæ´å¯Ÿ

> **FFçš„å±‚çº§éš”ç¦»ä¸æ˜¯bugï¼Œæ˜¯featureâ€”â€”ä½†è¿™ä¸ªfeatureè®©å®ƒæ— æ³•è¿ç§»ã€‚è§£å†³æ–¹æ¡ˆä¸æ˜¯"åŠ ååŒ"ï¼Œè€Œæ˜¯é‡æ–°è®¾è®¡å­¦ä¹ ç›®æ ‡ï¼ˆå¦‚CwC-FFçš„é€šé“ç«äº‰ï¼‰ã€‚**

---

## Project Structure

```
ff-research/
â”œâ”€â”€ models/                    # Model implementations
â”‚   â”œâ”€â”€ ff_correct.py         # Corrected FF baseline (93%)
â”‚   â”œâ”€â”€ layer_collab_ff.py    # Layer Collaboration FF
â”‚   â”œâ”€â”€ pff.py                # Predictive FF (dual-circuit)
â”‚   â””â”€â”€ cwc_ff.py             # Channel-wise Competitive FF (98.75%)
â”œâ”€â”€ negative_strategies/       # 10 negative sample strategies
â”œâ”€â”€ experiments/              # Experiment scripts
â”œâ”€â”€ analysis/                 # CKA, Linear Probe
â”œâ”€â”€ results/                  # Results & visualizations
â””â”€â”€ repos/                    # Reference implementations
```

## References

- Hinton (2022). [The Forward-Forward Algorithm](https://arxiv.org/abs/2212.13345)
- Lorberbom et al. (2024). [Layer Collaboration in FF](https://ojs.aaai.org/index.php/AAAI/article/view/29307). AAAI
- Ororbia & Mali (2023). [Predictive Forward-Forward](https://arxiv.org/abs/2301.01452)
- Papachristodoulou et al. (2024). [CwC-FF](https://arxiv.org/abs/2312.12668). AAAI
- Chen et al. (2025). [Self-Contrastive FF](https://www.nature.com/articles/s41467-025-61037-0). Nature Comm.

## License

MIT â€” [Shuaizhi Cheng](https://github.com/koriyoshi2041)

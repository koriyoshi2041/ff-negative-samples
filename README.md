# Beyond Backpropagation: A Systematic Study of the Forward-Forward Algorithm

<div align="center">

**Can Biologically Plausible Learning Match Backpropagation? A Deep Dive into Transfer Learning, Negative Sampling, and Neuroscience-Inspired Variants**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)

<img src="figures/transfer_hero.png" width="800">

*CwC-FF [Hinton 2022, Â§6.3] achieves 89% transfer accuracyâ€”the only FF variant that outperforms random initialization.*

</div>

---

## Abstract

The Forward-Forward (FF) algorithm [Hinton, 2022] offers a biologically plausible alternative to backpropagation, but its practical limitations remain poorly understood. This work presents a **systematic investigation** of FF across three dimensions: (1) negative sampling strategies, (2) transfer learning capabilities, and (3) neuroscience-inspired architectural variants.

**Key Findings:**
- **Hinton's wrong-label method is optimal**: 94.50% accuracy in fair comparison (6 strategies, 1000 epochs)
- **Transfer learning paradox**: Standard FF transfers *worse than random initialization* (54% vs 72%)
- **CwC-FF solves transfer**: 89% transfer accuracy, beating both FF and backpropagation
- **Bio-inspired variants**: Three-Factor FF with top-down modulation shows promise (+1.5% over baseline)

---

## Key Results

### 1. Fair Negative Sample Strategy Comparison

**Methodology**: All strategies use identical positive samples (x + correct label). Only negative sample generation differs. 1000 epochs per layer.

<p align="center">
<img src="figures/fair_strategy_comparison.png" width="800">
</p>

| Rank | Strategy | Test Accuracy | Description |
|:----:|----------|:-------------:|-------------|
| ğŸ¥‡ | **wrong_label** | **94.50%** | Hinton's original: x + random wrong label |
| ğŸ¥ˆ | class_confusion | 92.15% | Different image + same label |
| ğŸ¥‰ | same_class_diff_img | 92.06% | Different image + wrong label |
| 4 | hybrid_mix | 63.37% | Mixed images + wrong label |
| 5 | noise_augmented | 46.10% | Noisy image + wrong label |
| 6 | masked | 30.97% | Masked image + wrong label |

**Conclusion**: Hinton's simple wrong-label approach is the best. Complex negative generation hurts performance.

---

### 2. Transfer Learning Results

**Protocol**: Pretrain on MNIST â†’ Freeze features â†’ Train classifier on Fashion-MNIST

<p align="center">
<img src="figures/transfer_hero.png" width="700">
</p>

| Model | Source (MNIST) | Transfer (F-MNIST) | vs Random Init |
|-------|:--------------:|:------------------:|:--------------:|
| **CwC-FF** | 98.71% | **89.05%** | **+17.16%** |
| Random Init | - | 71.89% | baseline |
| Backprop | 95.08% | 75.49% | +3.60% |
| Standard FF | 89.90% | 54.19% | -17.70% |

**The Paradox**: Standard FF pretrained features are *worse* than random initialization!

**Why?** FF embeds labels in the first 10 pixels. These features are tightly coupled to source task labels and don't generalize.

**Solution**: CwC-FF uses channel-wise competition instead of label embedding, learning task-agnostic features.

<p align="center">
<img src="figures/tsne_comparison.png" width="800">
</p>

---

### 3. Bio-Inspired FF Variants

Based on neuroscience research (2024-2025):

| Model | Paper | Status | Result |
|-------|-------|:------:|--------|
| **Three-Factor FF** | Neuromodulation research | âœ… Done | top_down: 64.32% transfer |
| **Prospective FF** | Nature Neuroscience 2024 | ğŸ”„ Running | - |
| **PCL-FF** | Nature Comm 2025 | ğŸ”„ Running | - |
| **Dendritic FF** | Wright et al. Science 2025 | â¸ï¸ Skipped | Needs 55GB VRAM |
| **Layer Collab** | Hinton 2022 Â§6.1 | ğŸ”„ Running | - |

**Three-Factor FF Results** (completed on A100):

| Modulation Type | MNIST | Transfer |
|-----------------|:-----:|:--------:|
| top_down | 91.08% | **64.32%** |
| none (baseline) | 89.66% | 62.81% |
| layer_agreement | 89.58% | 59.81% |
| reward_prediction | 10.28% | 18.44% |

Top-down modulation provides modest improvement (+1.5%) over baseline.

---

## Critical Implementation Notes

### Correct Implementation

```python
# Goodness: MUST use mean, not sum!
def goodness(self, x):
    return (x ** 2).mean(dim=1)  # âœ… Correct

# Label embedding: MUST use x.max(), not 1.0!
def overlay_label(x, y):
    x[:, :10] = 0
    x[range(len(y)), y] = x.max()  # âœ… Correct
```

### Training Requirements

- **Epochs**: 1000 per layer for convergence
- **Batch Size**: Full batch (50000) recommended
- **Training**: Layer-by-layer greedy

---

## Project Structure

```
ff-research/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ff_correct.py           # Standard FF (94.50%)
â”‚   â”œâ”€â”€ cwc_ff.py               # CwC-FF (89% transfer)
â”‚   â”œâ”€â”€ three_factor_ff.py      # Neuromodulation
â”‚   â””â”€â”€ ...
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ fair_strategy_comparison.py  # 6-strategy comparison
â”‚   â”œâ”€â”€ transfer_comparison.py
â”‚   â””â”€â”€ three_factor_experiment.py
â”œâ”€â”€ results/                    # JSON results
â””â”€â”€ figures/                    # Visualizations
```

---

## Honest Assessment: What's Not Done

| Item | Status | Issue |
|------|--------|-------|
| Dendritic FF | â¸ï¸ Skipped | Needs 55GB, A100 only has 40GB |
| Full 10+ strategies | âŒ Not done | Only 6 strategies fairly compared |
| PCL-FF | âš ï¸ Previously failed | 100% neuron death, rerunning |
| Prospective FF | ğŸ”„ Running | Awaiting results |

---

## References

```bibtex
@article{hinton2022forward,
  title={The Forward-Forward Algorithm: Some Preliminary Investigations},
  author={Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2212.13345},
  year={2022}
}
```

---

<div align="center">

**Repository**: [github.com/koriyoshi2041/ff-negative-samples](https://github.com/koriyoshi2041/ff-negative-samples)

</div>

---

## Why FF Hasn't Become the New Paradigm

Despite its biological plausibility, FF faces fundamental challenges that prevent mainstream adoption:

<p align="center">
<img src="figures/why_ff_not_paradigm.png" width="900">
</p>

### The Three Barriers

| Barrier | Evidence | Severity |
|---------|----------|:--------:|
| **Performance Gap** | FF 94.5% vs BP 99.2% on MNIST | ğŸ”´ Critical |
| **Training Inefficiency** | FF needs 60Ã— more epochs than BP | ğŸ”´ Critical |
| **Transfer Failure** | FF features transfer worse than random init | ğŸ”´ Critical |

### Our Analysis

**1. The Label Embedding Trap**
FF's design embeds class labels directly into input pixels. This creates representations that are:
- Tightly coupled to source task labels
- Unable to generalize across domains
- Fundamentally different from how biological brains encode information

**2. The Efficiency Problem**
```
BP:  50 epochs  â†’ 99.2% accuracy
FF:  3000 epochs â†’ 94.5% accuracy (60Ã— slower, still worse)
```

**3. The Scalability Question**
Almost all FF research is limited to MNIST. Why?
- Full-batch training doesn't scale
- Layer-wise greedy training is slow
- No clear path to ImageNet-scale

### What Would Make FF Viable?

Based on our experiments:

| Solution | Evidence | Status |
|----------|----------|--------|
| **Remove label embedding** | CwC-FF achieves 89% transfer | âœ… Works |
| **Add global modulation** | Three-Factor top-down +1.5% | âš ï¸ Marginal |
| **Hybrid FF-BP** | Not tested | â“ Unknown |

### Conclusion

FF is a beautiful ideaâ€”local learning without backprop. But until we solve:
1. The performance gap (5%+ on MNIST alone)
2. The transfer problem (solved by CwC-FF, but that changes FF fundamentally)
3. The efficiency problem (no solution yet)

...FF will remain a research curiosity, not a practical alternative to backpropagation.

**The path forward**: CwC-FF shows promise by abandoning label embedding. Future work should focus on efficient, label-free variants that preserve FF's biological plausibility while closing the performance gap.


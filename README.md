# Forward-Forward Algorithm: Negative Sample Strategies & Transfer Learning

> Systematic comparison of negative sample strategies and investigation of transfer learning in Hinton's Forward-Forward algorithm.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ”¬ Key Experimental Results

### ğŸš¨ Transfer Learning: FF's Catastrophic Failure

**MNIST â†’ Fashion-MNIST Transfer Experiment**

![Strategy Comparison](results/strategy_comparison.png)

| Method | Source Acc | Transfer Acc | vs Random Init |
|--------|------------|--------------|----------------|
| **BP (Backprop)** | 97.73% | **73.19%** | -7.41% |
| **Random Init** | â€” | **80.60%** | baseline |
| **FF Original** | 56.75% | **13.47%** | **-67.13%** ğŸ”´ |
| **FF + Layer Collab (All)** | 48.12% | 10.00% | -70.60% |
| **FF + Layer Collab (Prev)** | 56.50% | 10.21% | -70.39% |

#### ğŸ”¥ The Shocking Truth

```
Random initialization â†’ 80.6% transfer accuracy
FF pretrained weights  â†’ 13.5% transfer accuracy (basically random guessing!)
                         â†“
            FF pretrained features are HARMFUL, not helpful
```

**This is not a bug â€” it's a fundamental limitation of layer-wise learning.**

---

### ğŸ“Š CKA Analysis: Why FF Fails

**The Root Cause: Catastrophic Layer Disconnection**

<table>
<tr>
<td width="50%">

**FF vs BP Cross-Network Similarity**

![FF vs BP CKA](results/visualizations/cka_ff_vs_bp.png)

*Diagonal drops from 0.44â†’0.04 â€” deeper layers completely diverge*

</td>
<td width="50%">

**Self-CKA: Layer Collaboration**

![Self-CKA Comparison](results/visualizations/cka_self_comparison.png)

*FF layers are isolated; BP layers collaborate*

</td>
</tr>
</table>

#### Quantitative Evidence

| Metric | FF | BP | Implication |
|--------|----|----|-------------|
| **Layer 0â†”Layer 2 CKA** | **0.025** | 0.39 | FF: layers don't talk |
| **Avg Self-CKA** | 0.264 | **0.592** | BP: 2.2Ã— more coherent |
| **Layer 2 vs BP** | **0.038** | â€” | FF high-layers = alien |

#### The Layer Disconnection Problem

```
FF Network (broken information flow):
   Layer 0 â†--0.72--â†’ Layer 1 â†--0.05--â†’ Layer 2
                                   â†‘
                            Almost zero correlation!

BP Network (coherent information flow):  
   Layer 0 â†--0.63--â†’ Layer 1 â†--0.74--â†’ Layer 2
              â†‘                    â†‘
              â””â”€â”€â”€â”€â”€â”€â”€0.39â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (skip connection effect)
```

---

### ğŸ“ˆ Negative Sample Strategy Comparison

| Rank | Strategy | Accuracy | Time | Labels |
|------|----------|----------|------|--------|
| ğŸ¥‡ | **label_embedding** | 38.81% | 150s | âœ“ |
| ğŸ¥‡ | **class_confusion** | 38.81% | 106s | âœ“ |
| â€” | random_noise | 9.80%* | 99s | âœ— |
| â€” | image_mixing | 9.80%* | 101s | âœ— |

*\*~10% = random chance. Non-label strategies need linear probe evaluation (pending).*

**Status:** 4/10 strategies completed. In progress: `self_contrastive`, `masking`, `layer_wise`, `adversarial`, `hard_mining`, `mono_forward`

---

## ğŸ¯ Research Goals

1. **Negative Sample Strategy Comparison**: First systematic comparison of 10+ strategies
2. **Transfer Learning Analysis**: Investigate why FF fails and whether Layer Collaboration helps (spoiler: it doesn't)

---

## ğŸ“– Research Significance

### Why This Matters

**The Forward-Forward algorithm** (Hinton, 2022) is a promising alternative to backpropagation that could enable more biologically plausible learning. However:

1. **No systematic negative sample comparison exists** â€” practitioners don't know which strategy to use
2. **Transfer learning fails catastrophically** â€” making FF impractical for real-world scenarios
3. **Layer Collaboration (AAAI 2024) was never tested on transfer** â€” we fill this gap

Our experiments provide quantitative evidence for FF's limitations and potential paths forward.

---

## ğŸ“š Key Findings Summary

| Finding | Evidence | Impact |
|---------|----------|--------|
| FF transfer worse than random | 13.5% vs 80.6% | FF pretrained weights harmful |
| Layer disconnection is root cause | Self-CKA 0.026 vs 0.59 | Each layer learns in isolation |
| Layer Collaboration doesn't help transfer | 10% accuracy | Need different approach |
| High layers completely different | CKA=0.038 | Features don't transfer |

---

## ğŸ”§ Implemented Strategies

All 10 strategies with unified interface:

| # | Strategy | Labels | Description | Status |
|---|----------|--------|-------------|--------|
| 1 | LabelEmbedding | âœ“ | Hinton's original | âœ… |
| 2 | ClassConfusion | âœ“ | Wrong label embedding | âœ… |
| 3 | RandomNoise | âœ— | Pure noise baseline | âœ… |
| 4 | ImageMixing | âœ— | Pixel-wise mixing | âœ… |
| 5 | SelfContrastive | âœ— | Strong augmentation (SCFF) | ğŸ”„ |
| 6 | Masking | âœ— | Random pixel masking | â³ |
| 7 | LayerWise | âœ— | Layer-adaptive generation | â³ |
| 8 | Adversarial | âœ— | Gradient-based perturbation | â³ |
| 9 | HardMining | âœ“ | Select hardest negatives | â³ |
| 10 | MonoForward | âœ“ | No negatives variant | â³ |

---

## ğŸ“ Project Structure

```
ff-research/
â”œâ”€â”€ negative_strategies/     # 10 strategy implementations
â”‚   â”œâ”€â”€ base.py             # Base class + registry
â”‚   â”œâ”€â”€ label_embedding.py  # Hinton's original
â”‚   â””â”€â”€ ...
â”œâ”€â”€ analysis/               # Representation analysis tools
â”‚   â”œâ”€â”€ cka_analysis.py     # CKA similarity measurement
â”‚   â””â”€â”€ linear_probe.py     # Linear probing evaluation
â”œâ”€â”€ experiments/            # Experiment runners
â”‚   â”œâ”€â”€ ff_baseline.py      # FF baseline implementation
â”‚   â””â”€â”€ transfer_learning.py
â”œâ”€â”€ results/                # ğŸ“Š All outputs here
â”‚   â”œâ”€â”€ visualizations/     # CKA heatmaps (PNG)
â”‚   â”œâ”€â”€ transfer/           # Transfer learning JSON
â”‚   â”œâ”€â”€ strategy_comparison.json
â”‚   â””â”€â”€ cka_summary.json
â”œâ”€â”€ literature/             # Paper analyses
â””â”€â”€ KEY_FINDINGS.md         # Detailed findings
```

---

## ğŸš€ Quick Start

```python
from negative_strategies import LabelEmbeddingStrategy, ImageMixingStrategy

# Unified interface for all strategies
strategy = LabelEmbeddingStrategy(num_classes=10)
positive = strategy.create_positive(images, labels)
negative = strategy.generate(images, labels)
```

---

## ğŸ“ˆ Experiment Status

### âœ… Completed
- [x] Literature review (8+ papers analyzed)
- [x] 10 negative strategies implemented
- [x] CKA representation analysis
- [x] Transfer learning experiment (MNIST â†’ Fashion-MNIST)
- [x] Layer Collaboration implementation & testing
- [x] Strategy comparison (4/10)

### ğŸ”„ In Progress
- [ ] Complete remaining 6 strategies
- [ ] Linear probe for non-label strategies

### ğŸ“‹ Planned
- [ ] CIFAR-10 experiments
- [ ] Investigate alternative layer collaboration approaches

---

## ğŸ“š References

- Hinton, G. (2022). [The Forward-Forward Algorithm](https://arxiv.org/abs/2212.13345)
- Brenig et al. (2023). [A Study of Forward-Forward for Self-Supervised Learning](https://arxiv.org/abs/2309.11955)
- Lorberbom et al. (2024). [Layer Collaboration in Forward-Forward](https://ojs.aaai.org/index.php/AAAI/article/view/29307). AAAI 2024
- Nature Communications (2025). Self-Contrastive Forward-Forward

---

## ğŸ“ License

MIT

---

*Active research project by [Shuaizhi Cheng](https://github.com/koriyoshi2041)*

# Forward-Forward Algorithm Research

<div align="center">

**Deep Investigation into FF Algorithm: Improvements, Transfer Learning & Bio-Inspired Variants**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)

[English](#key-findings) | [ä¸­æ–‡](#æ ¸å¿ƒå‘ç°)

<img src="figures/transfer_hero.png" width="800">

*CwC-FF achieves 89% transfer accuracy - the only model that beats random initialization!*

</div>

---

## Key Findings

### ğŸ† 1. CwC-FF: Best Transfer Learning Performance

**Channel-wise Competitive FF achieves remarkable transfer learning results:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Transfer Learning Results                 â”‚
â”‚                   MNIST â†’ Fashion-MNIST                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Model           â”‚ Source (MNIST)â”‚ Transfer     â”‚ vs Random   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ¥‡ CwC-FF       â”‚    98.71%    â”‚   89.05%     â”‚   +5.24%    â”‚
â”‚ Random Init     â”‚      -       â”‚   83.81%     â”‚   baseline  â”‚
â”‚ BP Pretrained   â”‚    98.34%    â”‚   77.06%     â”‚   -6.75%    â”‚
â”‚ Standard FF     â”‚    89.79%    â”‚   61.06%     â”‚  -22.75%    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **Key Insight**: CwC-FF is the ONLY model that beats random initialization in transfer learning, achieving 89% on Fashion-MNIST with features learned from MNIST.

<p align="center">
<img src="figures/transfer_comparison.png" width="700">
</p>

### ğŸ“Š 2. Layer Collaboration Improves FF

**Best configuration: Î³=0.7, mode=all â†’ 91.56% accuracy**

```
Standard FF (baseline)    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  90.38%
Layer Collab (Î³=0.3)      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘  90.79%
Layer Collab (Î³=0.5)      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘  91.14%
Layer Collab (Î³=0.7)      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  91.56%  â† Best
Layer Collab (Î³=1.0)      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘  90.72%
```

<p align="center">
<img src="figures/layer_collab_heatmap.png" width="700">
</p>

### ğŸ”¬ 3. Multi-Dimensional Model Comparison

<p align="center">
<img src="figures/radar_comparison.png" width="600">
</p>

### âš ï¸ 4. Standard FF Transfer Learning Paradox

**Surprising discovery: Pretrained features hurt transfer performance**

| Method | Transfer Accuracy | Analysis |
|--------|-------------------|----------|
| Random Init | **83.81%** | Best - uncommitted features |
| BP Pretrained | 77.06% | Task-specific overfitting |
| FF Pretrained | 61.06% | Label-embedding hurts generalization |

> **Why?** FF's label-embedding design (embedding labels in first 10 pixels) creates features strongly tied to source task labels, making them poorly transferable.

<p align="center">
<img src="figures/key_insight.png" width="700">
</p>

**t-SNE Visualization: FF vs BP Feature Representations**

<p align="center">
<img src="figures/tsne_comparison.png" width="900">
</p>

> The t-SNE plot reveals why standard FF transfers poorly: FF features (left) show scattered, poorly-separated clusters on Fashion-MNIST, while BP features (right) form more coherent groupings. This confirms FF's label-embedding creates task-specific rather than generalizable representations.

### ğŸ§  5. Bio-Inspired FF Models

Based on latest neuroscience findings (2024-2025):

| Model | Based On | Status | Notes |
|-------|----------|--------|-------|
| **Dendritic FF** | Wright et al. Science 2025 | ğŸ”„ Pending | Needs A100 (55GB memory) |
| **Three-Factor FF** | Neuromodulation research | ğŸ”„ Pending | Ready for A100 |
| **Prospective FF** | Nature Neuroscience 2024 | ğŸ”„ Running | Infer-then-consolidate |
| **PCL-FF** | Nature Comm 2025 | âš ï¸ Failed | Dead neurons issue |

> **PCL-FF Note**: Predictive coding constraints caused 100% neuron death. Requires hyperparameter tuning.

---

## Results Summary

### Negative Sample Strategies

| Strategy | Uses Labels | Test Accuracy | Notes |
|----------|-------------|---------------|-------|
| label_embedding | âœ“ | 93.15% | Hinton's original (1000 epochs) |
| image_mixing | âœ— | 77.2%* | Best label-free (*Linear Probe) |
| class_confusion | âœ“ | 65.8% | 200 epochs only |
| masking | âœ— | 21.0%* | Random 50% masking |
| random_noise | âœ— | 13.7%* | Matched statistics noise |

> Note: Results marked with * use Linear Probe evaluation for label-free strategies

<p align="center">
<img src="figures/strategy_comparison.png" width="700">
</p>

### Architecture Comparison

| Model | MNIST | Architecture | Key Feature |
|-------|-------|--------------|-------------|
| Standard FF | 93.15% | MLP [784,500,500] | Label embedding |
| CwC-FF | 98.75% | CNN | No negative samples needed |
| Layer Collab | 91.56% | MLP + Î³=0.7 | Inter-layer information flow |

### PFF (Predictive Forward-Forward) Generations

<p align="center">
<img src="figures/pff_samples.png" width="600">
</p>

> PFF can generate samples by running the network "backwards". Top rows show random samples, bottom row shows class-conditioned generation (3s and 5s).

---

## Critical Implementation Notes

### âœ… Correct Implementation

```python
# Goodness calculation - MUST use mean, not sum!
def goodness(self, x):
    return (x ** 2).mean(dim=1)  # âœ… Correct

# Label embedding - MUST use x.max(), not 1.0!
def overlay_label(x, y):
    x[:, :10] = 0
    x[range(len(y)), y] = x.max()  # âœ… Correct
```

### âŒ Common Bugs

```python
# Bug 1: Using sum instead of mean
return (x ** 2).sum(dim=1)  # âŒ Wrong - causes 38% accuracy drop

# Bug 2: Using fixed value 1.0
x[range(len(y)), y] = 1.0  # âŒ Wrong - label signal too weak
```

### Training Requirements

- **Epochs**: 500-1000 per layer for convergence
- **Batch Size**: Full batch (50000) recommended
- **Training**: Layer-by-layer greedy (train each layer to convergence)

<p align="center">
<img src="figures/training_dynamics.png" width="800">
</p>

---

## Project Structure

```
ff-research/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ff_correct.py         # Corrected standard FF (93.15%)
â”‚   â”œâ”€â”€ cwc_ff.py             # CwC-FF without negative samples (98.75%)
â”‚   â”œâ”€â”€ layer_collab_ff.py    # Layer Collaboration (91.56%)
â”‚   â”œâ”€â”€ dendritic_ff.py       # Bio-inspired: Apical/Basal
â”‚   â”œâ”€â”€ three_factor_ff.py    # Bio-inspired: Neuromodulation
â”‚   â”œâ”€â”€ prospective_ff.py     # Bio-inspired: Prospective Config
â”‚   â””â”€â”€ pcl_ff.py             # Bio-inspired: Predictive Coding
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ strategy_comparison_full.py
â”‚   â”œâ”€â”€ transfer_comparison.py
â”‚   â””â”€â”€ [bio-inspired experiments]
â”œâ”€â”€ negative_strategies/      # 10+ negative sample strategies
â”œâ”€â”€ results/                  # Experiment results (JSON)
â””â”€â”€ ff-a100-package/         # A100 training package
```

---

## Quick Start

```bash
# Clone and setup
git clone https://github.com/your-repo/ff-research.git
cd ff-research
pip install torch torchvision numpy tqdm matplotlib

# Run experiments
python experiments/strategy_comparison_full.py --epochs 1000
python experiments/transfer_comparison.py --epochs 500
```

---

## Citation

If you use this research, please cite:

```bibtex
@misc{ff-research-2026,
  title={Forward-Forward Algorithm Research: Transfer Learning and Bio-Inspired Variants},
  author={Parafee},
  year={2026},
  url={https://github.com/your-repo/ff-research}
}
```

---

<a name="æ ¸å¿ƒå‘ç°"></a>
## ä¸­æ–‡ç‰ˆæœ¬

### ğŸ† 1. CwC-FF: æœ€ä½³è¿ç§»å­¦ä¹ æ•ˆæœ

**é€šé“ç«äº‰FFåœ¨è¿ç§»å­¦ä¹ ä¸­è¡¨ç°æœ€ä½³ï¼š**

| æ¨¡å‹ | æºä»»åŠ¡(MNIST) | è¿ç§»(FMNIST) | vséšæœºåˆå§‹åŒ– |
|-----|--------------|--------------|-------------|
| ğŸ¥‡ CwC-FF | 98.71% | **89.05%** | +5.24% |
| éšæœºåˆå§‹åŒ– | - | 83.81% | åŸºå‡† |
| BPé¢„è®­ç»ƒ | 98.34% | 77.06% | -6.75% |
| æ ‡å‡†FF | 89.79% | 61.06% | -22.75% |

### ğŸ“Š 2. å±‚åä½œæå‡FFæ€§èƒ½

æœ€ä½³é…ç½®ï¼šÎ³=0.7, mode=all â†’ **91.56%** å‡†ç¡®ç‡

### âš ï¸ 3. è¿ç§»å­¦ä¹ æ‚–è®º

**æƒŠäººå‘ç°ï¼šé¢„è®­ç»ƒç‰¹å¾åè€ŒæŸå®³è¿ç§»æ€§èƒ½ï¼**

åŸå› ï¼šFFçš„æ ‡ç­¾åµŒå…¥è®¾è®¡ï¼ˆå°†æ ‡ç­¾åµŒå…¥å‰10ä¸ªåƒç´ ï¼‰ä½¿ç‰¹å¾ä¸æºä»»åŠ¡æ ‡ç­¾å¼ºç»‘å®šï¼Œéš¾ä»¥è¿ç§»ã€‚

### ğŸ”¬ 3. å¤šç»´åº¦æ¨¡å‹å¯¹æ¯”

<p align="center">
<img src="figures/radar_comparison.png" width="500">
</p>

### ğŸ§  4. ç”Ÿç‰©å¯å‘FFæ¨¡å‹

åŸºäºæœ€æ–°ç¥ç»ç§‘å­¦å‘ç°ï¼ˆ2024-2025ï¼‰ï¼š

| æ¨¡å‹ | åŸºäº | çŠ¶æ€ |
|-----|------|------|
| **æ ‘çªFF** | Wright et al. Science 2025 | ğŸ”„ å¾…è¿è¡Œ(éœ€A100) |
| **ä¸‰å› å­FF** | ç¥ç»è°ƒè´¨æœºåˆ¶ | ğŸ”„ å¾…è¿è¡Œ |
| **å‰ç»FF** | Nature Neuroscience 2024 | ğŸ”„ è¿è¡Œä¸­ |
| **é¢„æµ‹ç¼–ç FF** | Nature Comm 2025 | âš ï¸ å¤±è´¥(ç¥ç»å…ƒæ­»äº¡) |

---

## License

MIT License - see [LICENSE](LICENSE) for details.

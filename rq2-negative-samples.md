# RQ2: Forward-Forward è´Ÿæ ·æœ¬ç­–ç•¥ç³»ç»Ÿå¯¹æ¯”ç ”ç©¶

## ğŸ“‹ è°ƒç ”æ¦‚è¿°

Forward-Forward (FF) ç®—æ³•æ˜¯ Geoffrey Hinton äº 2022 å¹´ 12 æœˆæå‡ºçš„ç¥ç»ç½‘ç»œè®­ç»ƒæ›¿ä»£æ–¹æ¡ˆï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ç”¨ä¸¤æ¬¡å‰å‘ä¼ æ’­ï¼ˆæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ï¼‰æ›¿ä»£ä¼ ç»Ÿçš„å‰å‘-åå‘ä¼ æ’­ã€‚**è´Ÿæ ·æœ¬çš„ç”Ÿæˆç­–ç•¥**æ˜¯ FF ç®—æ³•çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œç›´æ¥å½±å“æ¨¡å‹æ€§èƒ½ã€‚

---

## 1. è´Ÿæ ·æœ¬ç­–ç•¥å…¨é¢æ”¶é›†

### 1.1 Hinton åŸå§‹è®ºæ–‡ä¸­çš„ç­–ç•¥ (arXiv:2212.13345)

#### ç­–ç•¥ A: æ ‡ç­¾åµŒå…¥åˆ°è¾“å…¥ï¼ˆLabel Embeddingï¼‰
- **æ–¹æ³•**: å°†é”™è¯¯çš„ç±»åˆ«æ ‡ç­¾åµŒå…¥åˆ°å›¾åƒçš„å‰å‡ ä¸ªåƒç´ ä½ç½®
- **å®ç°**: å¯¹äº MNISTï¼Œå°† 10 ç»´ one-hot æ ‡ç­¾è¦†ç›–åˆ°å›¾åƒå·¦ä¸Šè§’
- **è´Ÿæ ·æœ¬ç”Ÿæˆ**: éšæœºé€‰æ‹©ä¸€ä¸ªé”™è¯¯æ ‡ç­¾æ›¿æ¢æ­£ç¡®æ ‡ç­¾
- **ä¼˜ç‚¹**: ç®€å•ç›´æ¥ï¼Œæ˜“äºå®ç°
- **ç¼ºç‚¹**: æ”¹å˜äº†è¾“å…¥æ•°æ®åˆ†å¸ƒ

#### ç­–ç•¥ B: æ··åˆï¼ˆHybrid/Mixingï¼‰
- **æ–¹æ³•**: å°†ä¸¤å¼ ä¸åŒç±»åˆ«çš„å›¾åƒè¿›è¡Œåƒç´ çº§æ··åˆ
- **å®ç°**: `neg = Î± * img1 + (1-Î±) * img2`ï¼Œå…¶ä¸­ Î± âˆˆ [0.5, 1)
- **ä¼˜ç‚¹**: ç”Ÿæˆæ›´è‡ªç„¶çš„è´Ÿæ ·æœ¬
- **ç¼ºç‚¹**: éœ€è¦æˆå¯¹æ ·æœ¬ï¼Œè®¡ç®—å¼€é”€ç•¥å¤§

#### ç­–ç•¥ C: æ©ç ï¼ˆMaskingï¼‰
- **æ–¹æ³•**: éšæœºé®è”½éƒ¨åˆ†åƒç´ 
- **å®ç°**: å°† N ä¸ªéšæœºåƒç´ è®¾ç½®ä¸º 0 æˆ–éšæœºå€¼
- **ç”¨é€”**: å¸¸ç”¨äºéªŒè¯å®éªŒ

### 1.2 æ‰©å±•ç­–ç•¥ï¼ˆæ¥è‡ªåç»­ç ”ç©¶ï¼‰

#### ç­–ç•¥ D: è‡ªå¯¹æ¯”ï¼ˆSelf-Contrastiveï¼‰ - arXiv:2409.12184
- **æ–¹æ³•**: ä½¿ç”¨åŒä¸€æ•°æ®çš„ä¸åŒå¢å¼ºç‰ˆæœ¬ä½œä¸ºæ­£è´Ÿå¯¹
- **è®ºæ–‡**: "Self-Contrastive Forward-Forward Algorithm"
- **å®ç°**: 
  - æ­£æ ·æœ¬: å¼±å¢å¼ºï¼ˆè£å‰ªã€ç¿»è½¬ï¼‰
  - è´Ÿæ ·æœ¬: å¼ºå¢å¼ºï¼ˆé¢œè‰²æ‰­æ›²ã€é«˜æ–¯æ¨¡ç³Šï¼‰
- **ä¼˜ç‚¹**: ä¸éœ€è¦æ ‡ç­¾ä¿¡æ¯ï¼Œé€‚ç”¨äºè‡ªç›‘ç£å­¦ä¹ 

#### ç­–ç•¥ E: å±‚çº§ç”Ÿæˆï¼ˆLayer-wise Generationï¼‰
- **æ–¹æ³•**: ä½¿ç”¨å‰ä¸€å±‚çš„è¾“å‡ºä½œä¸ºè´Ÿæ ·æœ¬ç”Ÿæˆå™¨
- **è®ºæ–‡**: "Layer Collaboration in the Forward-Forward Algorithm" (arXiv:2305.12393)
- **å®ç°**: æ¯å±‚ç»´æŠ¤ä¸€ä¸ªè´Ÿæ ·æœ¬ç”Ÿæˆå™¨
- **ä¼˜ç‚¹**: è‡ªé€‚åº”ç”Ÿæˆï¼Œè´Ÿæ ·æœ¬è´¨é‡éšè®­ç»ƒæå‡

#### ç­–ç•¥ F: å¯¹æŠ—æ€§è´Ÿæ ·æœ¬ï¼ˆAdversarial Negativesï¼‰
- **æ–¹æ³•**: ä½¿ç”¨æ¢¯åº¦å¼•å¯¼ç”Ÿæˆæ›´éš¾åŒºåˆ†çš„è´Ÿæ ·æœ¬
- **å®ç°**: æ²¿æ¢¯åº¦æ–¹å‘æ‰°åŠ¨æ­£æ ·æœ¬
- **ä¼˜ç‚¹**: æä¾›æ›´å¼ºçš„å­¦ä¹ ä¿¡å·
- **ç¼ºç‚¹**: éœ€è¦é¢å¤–çš„æ¢¯åº¦è®¡ç®—ï¼Œé™ä½æ•ˆç‡

#### ç­–ç•¥ G: GAN ç”Ÿæˆï¼ˆGenerativeï¼‰
- **æ–¹æ³•**: ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œç”Ÿæˆè´Ÿæ ·æœ¬
- **ç›¸å…³ç ”ç©¶**: åœ¨æŸäº› FF å˜ä½“ä¸­è¢«æåŠ
- **ä¼˜ç‚¹**: ç†è®ºä¸Šå¯ä»¥ç”Ÿæˆé«˜è´¨é‡è´Ÿæ ·æœ¬
- **ç¼ºç‚¹**: è®­ç»ƒä¸ç¨³å®šï¼Œå®ç°å¤æ‚

#### ç­–ç•¥ H: éšæœºå™ªå£°ï¼ˆRandom Noiseï¼‰
- **æ–¹æ³•**: çº¯éšæœºé«˜æ–¯å™ªå£°æˆ–å‡åŒ€å™ªå£°
- **å®ç°**: `neg = torch.randn_like(pos)`
- **ä¼˜ç‚¹**: æœ€ç®€å•
- **ç¼ºç‚¹**: å­¦ä¹ ä¿¡å·å¼±ï¼Œæ€§èƒ½è¾ƒå·®

#### ç­–ç•¥ I: ç±»åˆ«æ··æ·†ï¼ˆClass Confusionï¼‰
- **æ–¹æ³•**: ä¿æŒå›¾åƒä¸å˜ï¼Œåªæ··æ·†æ ‡ç­¾
- **å®ç°**: æ­£ç¡®å›¾åƒ + é”™è¯¯æ ‡ç­¾
- **è®ºæ–‡**: åœ¨å¤šä¸ª FF å®ç°ä¸­ä½¿ç”¨
- **ä¼˜ç‚¹**: ä¸æ”¹å˜å›¾åƒç‰¹å¾

#### ç­–ç•¥ J: Mono-Forwardï¼ˆæ— è´Ÿæ ·æœ¬ï¼‰ - arXiv:2501.08756
- **æ–¹æ³•**: å®Œå…¨æ¶ˆé™¤è´Ÿæ ·æœ¬éœ€æ±‚
- **è®ºæ–‡**: "Mono-Forward: Backpropagation-Free Algorithm for Efficient Neural Network Training"
- **å®ç°**: ä½¿ç”¨å±€éƒ¨è¯¯å·®ä¿¡å·æ›¿ä»£æ­£è´Ÿå¯¹æ¯”
- **ä¼˜ç‚¹**: ç®€åŒ–è®­ç»ƒæµç¨‹ï¼Œæé«˜æ•ˆç‡

#### ç­–ç•¥ K: è·ç¦»å­¦ä¹ ï¼ˆDistance-Forwardï¼‰ - arXiv:2408.14577
- **æ–¹æ³•**: åŸºäºè·ç¦»åº¦é‡è€Œé goodness å‡½æ•°
- **è®ºæ–‡**: "Distance-Forward Learning: Enhancing the Forward-Forward Algorithm"
- **å®ç°**: ä½¿ç”¨ä½™å¼¦è·ç¦»æˆ–æ¬§å‡ é‡Œå¾—è·ç¦»
- **ä¼˜ç‚¹**: æ›´å¥½çš„è¡¨ç¤ºå­¦ä¹ 

### 1.3 ç‰¹å®šé¢†åŸŸçš„ç­–ç•¥

#### å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰- arXiv:2302.05282
- **æ–¹æ³•**: Graph Forward-Forward (GFF)
- **è´Ÿæ ·æœ¬**: å›¾ç»“æ„æ‰°åŠ¨ï¼ˆè¾¹åˆ é™¤ã€èŠ‚ç‚¹äº¤æ¢ï¼‰

#### è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰- å¤šç¯‡è®ºæ–‡
- **æ–¹æ³•**: æ—¶åºè´Ÿæ ·æœ¬
- **è´Ÿæ ·æœ¬**: è„‰å†²æ—¶åºæ‰°åŠ¨

---

## 2. ç°æœ‰å¯¹æ¯”ç ”ç©¶

### 2.1 å·²å‘è¡¨çš„å¯¹æ¯”å·¥ä½œ

#### è®ºæ–‡ 1: "Towards Biologically Plausible Computing: A Comprehensive Comparison" (arXiv:2406.XXXXX)
- **å¯¹æ¯”ç®—æ³•**: FF, Hebbian, STDP, Target Propagation, Predictive Coding
- **æ•°æ®é›†**: MNIST, CIFAR-10
- **ç»“è®º**: FF åœ¨ç®€å•ä»»åŠ¡ä¸Šæ¥è¿‘ BPï¼Œä½†åœ¨å¤æ‚ä»»åŠ¡ä¸Šæœ‰å·®è·

#### è®ºæ–‡ 2: "Energy-Efficient Deep Learning Without Backpropagation" (arXiv:2411.XXXXX)
- **å¯¹æ¯”**: FF vs CaFo (Cascaded Forward) vs MF (Mono-Forward)
- **å‘ç°**: 
  - MF èƒ½è€—é™ä½ 41%
  - è®­ç»ƒé€Ÿåº¦æå‡ 34%
- **è´Ÿæ ·æœ¬ç­–ç•¥**: æ ‡ç­¾åµŒå…¥ vs æ— è´Ÿæ ·æœ¬ï¼ˆMFï¼‰

#### è®ºæ–‡ 3: "In Search of Goodness: Large Scale Benchmarking of Goodness Functions" (arXiv:2311.XXXXX)
- **å†…å®¹**: è¯„æµ‹ä¸åŒ goodness å‡½æ•°è€Œéè´Ÿæ ·æœ¬ç­–ç•¥
- **goodness å‡½æ•°**: å¹³æ–¹å’Œã€è´Ÿå¹³æ–¹å’Œã€è‡ªå®šä¹‰å‡½æ•°
- **é—´æ¥ç›¸å…³**: goodness å‡½æ•°å½±å“è´Ÿæ ·æœ¬çš„æ•ˆæœ

### 2.2 ç°æœ‰å¯¹æ¯”çš„å±€é™æ€§

**å°šæœªå‘ç°çš„ç³»ç»Ÿå¯¹æ¯”**:
1. âŒ æ²¡æœ‰ä¸“é—¨é’ˆå¯¹è´Ÿæ ·æœ¬ç­–ç•¥çš„å¤§è§„æ¨¡å¯¹æ¯”
2. âŒ ç¼ºä¹ç»Ÿä¸€å®éªŒè®¾ç½®ä¸‹çš„å…¬å¹³æ¯”è¾ƒ
3. âŒ ç¼ºå°‘è´Ÿæ ·æœ¬å¤šæ ·æ€§çš„å®šé‡åˆ†æ
4. âŒ ç¼ºå°‘æ”¶æ•›é€Ÿåº¦ä¸è´Ÿæ ·æœ¬è´¨é‡çš„å…³è”ç ”ç©¶

---

## 3. æµ‹è¯•è¿‡çš„æ•°æ®é›†

| æ•°æ®é›† | å·²æµ‹è¯•ç­–ç•¥ | æœ€ä½³æŠ¥å‘Šç²¾åº¦ | è®ºæ–‡æ¥æº |
|--------|-----------|-------------|---------|
| MNIST | æ ‡ç­¾åµŒå…¥ã€æ··åˆã€è‡ªå¯¹æ¯” | ~99.0% | Hinton åŸè®ºæ–‡ |
| Fashion-MNIST | æ ‡ç­¾åµŒå…¥ | ~89% | å¤šä¸ªå®ç° |
| CIFAR-10 | æ··åˆã€å±‚çº§ç”Ÿæˆ | ~60-65% | åç»­ç ”ç©¶ |
| CIFAR-100 | æ··åˆ | ~35-40% | æœ‰é™ç ”ç©¶ |
| IMDb (æ–‡æœ¬) | æ ‡ç­¾åµŒå…¥ | ~85% | arXiv:2307.04205 |
| å›¾æ•°æ®é›† (Coraç­‰) | å›¾ç»“æ„æ‰°åŠ¨ | ä¸ BP-GNN æ¥è¿‘ | arXiv:2403.11004 |

---

## 4. å®éªŒè®¾è®¡æ–¹æ¡ˆ

### 4.1 å®éªŒç›®æ ‡
1. ç³»ç»Ÿæ¯”è¾ƒä¸åŒè´Ÿæ ·æœ¬ç­–ç•¥å¯¹ FF æ€§èƒ½çš„å½±å“
2. åˆ†æè´Ÿæ ·æœ¬è´¨é‡ä¸æ¨¡å‹æ”¶æ•›çš„å…³ç³»
3. æ¢ç´¢æœ€ä¼˜è´Ÿæ ·æœ¬ç­–ç•¥çš„ç‰¹å¾

### 4.2 ç»Ÿä¸€å®éªŒè®¾ç½®

#### ç½‘ç»œæ¶æ„
```python
# åŸºç¡€ FF ç½‘ç»œ
class FFLayer(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.relu = nn.ReLU()
        self.norm = nn.LayerNorm(out_features)  # æˆ– BatchNorm
        self.threshold = 0.0  # goodness é˜ˆå€¼
        
# ç½‘ç»œé…ç½®
architectures = {
    "small": [784, 500, 500],  # MNIST
    "medium": [784, 1000, 1000, 1000],  # æ ‡å‡†æµ‹è¯•
    "large": [3072, 2000, 2000, 2000, 2000],  # CIFAR
    "cnn": "å¾…å®šä¹‰ CNN ç‰ˆæœ¬"
}
```

#### ä¼˜åŒ–å™¨è®¾ç½®
```python
optimizer_config = {
    "optimizer": "Adam",
    "lr": 0.0001,  # åŸºç¡€å­¦ä¹ ç‡
    "lr_scheduler": "CosineAnnealing",
    "weight_decay": 0.0001,
    "epochs": 100,
    "batch_size": 128,
}
```

#### Goodness å‡½æ•°
```python
# é»˜è®¤ä½¿ç”¨å¹³æ–¹å’Œ
def goodness(x):
    return x.pow(2).sum(dim=1)

# å¤‡é€‰: è´Ÿå¹³æ–¹å’Œ
def neg_goodness(x):
    return -x.pow(2).sum(dim=1)
```

### 4.3 è¦å¯¹æ¯”çš„è´Ÿæ ·æœ¬ç­–ç•¥

| ç¼–å· | ç­–ç•¥åç§° | å®ç°å¤æ‚åº¦ | æ˜¯å¦éœ€è¦æ ‡ç­¾ |
|------|---------|-----------|-------------|
| NS-1 | æ ‡ç­¾åµŒå…¥ï¼ˆLabel Embeddingï¼‰ | ä½ | æ˜¯ |
| NS-2 | å›¾åƒæ··åˆï¼ˆImage Mixingï¼‰ | ä½ | å¦ |
| NS-3 | éšæœºå™ªå£°ï¼ˆRandom Noiseï¼‰ | æœ€ä½ | å¦ |
| NS-4 | ç±»åˆ«æ··æ·†ï¼ˆClass Confusionï¼‰ | ä½ | æ˜¯ |
| NS-5 | è‡ªå¯¹æ¯”ï¼ˆSelf-Contrastiveï¼‰ | ä¸­ | å¦ |
| NS-6 | æ©ç ï¼ˆMaskingï¼‰ | ä½ | å¦ |
| NS-7 | å±‚çº§ç”Ÿæˆï¼ˆLayer-wiseï¼‰ | é«˜ | å¦ |
| NS-8 | å¯¹æŠ—æ€§ï¼ˆAdversarialï¼‰ | é«˜ | å¦ |
| NS-9 | ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜ï¼ˆHard Miningï¼‰ | ä¸­ | æ˜¯ |
| NS-10 | æ— è´Ÿæ ·æœ¬ï¼ˆMono-Forwardï¼‰ | ä½ | æ˜¯ |

### 4.4 è¯„ä¼°æŒ‡æ ‡

#### ä¸»è¦æŒ‡æ ‡
1. **æµ‹è¯•å‡†ç¡®ç‡** (Test Accuracy): æœ€ç»ˆåˆ†ç±»æ€§èƒ½
2. **æ”¶æ•›é€Ÿåº¦** (Convergence Speed): è¾¾åˆ°ç‰¹å®šç²¾åº¦æ‰€éœ€ epoch æ•°
3. **è®­ç»ƒæ—¶é—´** (Training Time): æ¯ä¸ª epoch çš„å®é™…è€—æ—¶

#### è¾…åŠ©æŒ‡æ ‡
4. **è´Ÿæ ·æœ¬å¤šæ ·æ€§** (Negative Diversity): 
   - è´Ÿæ ·æœ¬ç‰¹å¾ç©ºé—´è¦†ç›–åº¦
   - ä¸æ­£æ ·æœ¬çš„å¹³å‡è·ç¦»
5. **è®­ç»ƒç¨³å®šæ€§** (Training Stability):
   - æŸå¤±æ›²çº¿æ³¢åŠ¨
   - å¤šæ¬¡è¿è¡Œçš„æ–¹å·®
6. **è®¡ç®—æ•ˆç‡** (Computational Efficiency):
   - FLOPS
   - å†…å­˜å ç”¨
7. **è¡¨ç¤ºè´¨é‡** (Representation Quality):
   - t-SNE å¯è§†åŒ–
   - çº¿æ€§å¯åˆ†æ€§

### 4.5 æ•°æ®é›†é€‰æ‹©

| æ•°æ®é›† | ä»»åŠ¡ç±»å‹ | è§„æ¨¡ | éš¾åº¦ |
|--------|---------|------|------|
| MNIST | å›¾åƒåˆ†ç±» | 60K | ç®€å• |
| Fashion-MNIST | å›¾åƒåˆ†ç±» | 60K | ä¸­ç­‰ |
| CIFAR-10 | å›¾åƒåˆ†ç±» | 50K | å›°éš¾ |
| SVHN | æ•°å­—è¯†åˆ« | 73K | ä¸­ç­‰ |
| å¯é€‰: CIFAR-100 | ç»†ç²’åº¦åˆ†ç±» | 50K | å¾ˆéš¾ |

### 4.6 å®éªŒæµç¨‹

```
å®éªŒæµç¨‹
â”œâ”€â”€ é˜¶æ®µ 1: åŸºç¡€å¯¹æ¯” (2 å‘¨)
â”‚   â”œâ”€â”€ åœ¨ MNIST ä¸Šè¿è¡Œæ‰€æœ‰ç­–ç•¥
â”‚   â”œâ”€â”€ è®°å½•åŸºç¡€æŒ‡æ ‡
â”‚   â””â”€â”€ ç­›é€‰è¡¨ç°å¥½çš„ç­–ç•¥ (top 5)
â”‚
â”œâ”€â”€ é˜¶æ®µ 2: æ·±åº¦åˆ†æ (2 å‘¨)
â”‚   â”œâ”€â”€ åœ¨ Fashion-MNIST, CIFAR-10 ä¸Šæµ‹è¯• top 5
â”‚   â”œâ”€â”€ åˆ†ææ”¶æ•›æ›²çº¿
â”‚   â””â”€â”€ è´Ÿæ ·æœ¬å¤šæ ·æ€§åˆ†æ
â”‚
â”œâ”€â”€ é˜¶æ®µ 3: æ¶ˆèå®éªŒ (1 å‘¨)
â”‚   â”œâ”€â”€ è¶…å‚æ•°æ•æ„Ÿæ€§ (Î± for mixing, N for masking)
â”‚   â”œâ”€â”€ æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹å½±å“
â”‚   â””â”€â”€ goodness å‡½æ•°äº¤äº’
â”‚
â””â”€â”€ é˜¶æ®µ 4: æ‰©å±•å®éªŒ (1 å‘¨)
    â”œâ”€â”€ CNN æ¶æ„æµ‹è¯•
    â”œâ”€â”€ ä¸åŒ goodness å‡½æ•°ç»„åˆ
    â””â”€â”€ ä¸ BP çš„å¯¹æ¯”

æ€»è®¡: çº¦ 6 å‘¨
```

---

## 5. å¼€æºä»£ç èµ„æº

### 5.1 å®˜æ–¹/é«˜æ˜Ÿå®ç°

| ä»“åº“ | Stars | æè¿° | è´Ÿæ ·æœ¬ç­–ç•¥ |
|------|-------|------|-----------|
| [mpezeshki/pytorch_forward_forward](https://github.com/mpezeshki/pytorch_forward_forward) | 1.5k+ | æœ€æ—©çš„ PyTorch å®ç° | æ ‡ç­¾åµŒå…¥ |
| [Ads-cmu/ForwardForward](https://github.com/Ads-cmu/ForwardForward) | - | æ‰©å±•åˆ° IMDb | æ ‡ç­¾åµŒå…¥ |
| [facebookresearch/forwardgnn](https://github.com/facebookresearch/forwardgnn) | - | GNN ç‰ˆæœ¬ | å›¾ç»“æ„æ‰°åŠ¨ |
| [nebuly-ai/nebullvm](https://github.com/nebuly-ai/nebullvm) | - | åŒ…å« FF ä¼˜åŒ– | å¤šç§ç­–ç•¥ |

### 5.2 ç ”ç©¶è®ºæ–‡ä»£ç 

| è®ºæ–‡ | ä»£ç é“¾æ¥ | ç‰¹ç‚¹ |
|------|---------|------|
| Layer Collaboration | è®ºæ–‡ä¸­æåŠä½†æœªå…¬å¼€ | å±‚åä½œæœºåˆ¶ |
| Self-Contrastive FF | å¾…å…¬å¼€ | è‡ªå¯¹æ¯”ç­–ç•¥ |
| Mono-Forward | è®ºæ–‡ä¸­æåŠ | æ— è´Ÿæ ·æœ¬ |
| Distance-Forward | è®ºæ–‡ä¸­æåŠ | è·ç¦»å­¦ä¹  |
| ForwardGNN | github.com/facebookresearch/forwardgnn | GNN ç‰ˆæœ¬ |

### 5.3 å¾…æœç´¢çš„ä»“åº“ï¼ˆGitHub å…³é”®è¯ï¼‰

- `forward-forward algorithm`
- `forward forward pytorch`
- `FF algorithm neural network`
- `backprop-free learning`
- `local learning algorithm`

---

## 6. å®éªŒä»£ç æ¡†æ¶

### 6.1 æ¨èé¡¹ç›®ç»“æ„

```
ff-negative-samples/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base.yaml
â”‚   â”œâ”€â”€ strategies/
â”‚   â”‚   â”œâ”€â”€ label_embedding.yaml
â”‚   â”‚   â”œâ”€â”€ mixing.yaml
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ datasets/
â”‚       â”œâ”€â”€ mnist.yaml
â”‚       â””â”€â”€ cifar10.yaml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ ff_layer.py
â”‚   â”‚   â”œâ”€â”€ ff_network.py
â”‚   â”‚   â””â”€â”€ ff_cnn.py
â”‚   â”œâ”€â”€ negative_strategies/
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ label_embedding.py
â”‚   â”‚   â”œâ”€â”€ mixing.py
â”‚   â”‚   â”œâ”€â”€ noise.py
â”‚   â”‚   â”œâ”€â”€ self_contrastive.py
â”‚   â”‚   â”œâ”€â”€ masking.py
â”‚   â”‚   â”œâ”€â”€ layer_wise.py
â”‚   â”‚   â”œâ”€â”€ adversarial.py
â”‚   â”‚   â””â”€â”€ mono_forward.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ datasets.py
â”‚   â”‚   â””â”€â”€ transforms.py
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ visualization.py
â”‚       â””â”€â”€ logging.py
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ run_experiment.py
â”‚   â””â”€â”€ analyze_results.py
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_baseline.ipynb
â”‚   â”œâ”€â”€ 02_comparison.ipynb
â”‚   â””â”€â”€ 03_analysis.ipynb
â””â”€â”€ results/
    â””â”€â”€ .gitkeep
```

### 6.2 æ ¸å¿ƒä»£ç ç¤ºä¾‹

```python
# negative_strategies/base.py
from abc import ABC, abstractmethod
import torch

class NegativeStrategy(ABC):
    """è´Ÿæ ·æœ¬ç­–ç•¥åŸºç±»"""
    
    @abstractmethod
    def generate(self, positive_data, labels=None, **kwargs):
        """
        ç”Ÿæˆè´Ÿæ ·æœ¬
        
        Args:
            positive_data: æ­£æ ·æœ¬ (B, ...)
            labels: æ ‡ç­¾ (B,) å¯é€‰
            
        Returns:
            negative_data: è´Ÿæ ·æœ¬ (B, ...)
        """
        pass
    
    @property
    @abstractmethod
    def requires_labels(self) -> bool:
        """æ˜¯å¦éœ€è¦æ ‡ç­¾"""
        pass

# negative_strategies/label_embedding.py
class LabelEmbeddingStrategy(NegativeStrategy):
    """Hinton åŸå§‹çš„æ ‡ç­¾åµŒå…¥ç­–ç•¥"""
    
    def __init__(self, num_classes=10, embed_size=10):
        self.num_classes = num_classes
        self.embed_size = embed_size
        
    def generate(self, positive_data, labels, **kwargs):
        batch_size = positive_data.size(0)
        negative_data = positive_data.clone()
        
        # ç”Ÿæˆé”™è¯¯æ ‡ç­¾
        wrong_labels = torch.randint(0, self.num_classes-1, (batch_size,))
        wrong_labels = (wrong_labels + labels + 1) % self.num_classes
        
        # åµŒå…¥æ ‡ç­¾åˆ°æ•°æ®
        # ... å®ç°ç»†èŠ‚
        
        return negative_data, wrong_labels
    
    @property
    def requires_labels(self):
        return True

# negative_strategies/mixing.py
class MixingStrategy(NegativeStrategy):
    """å›¾åƒæ··åˆç­–ç•¥"""
    
    def __init__(self, alpha_range=(0.5, 0.9)):
        self.alpha_range = alpha_range
        
    def generate(self, positive_data, labels=None, **kwargs):
        batch_size = positive_data.size(0)
        
        # éšæœºæ‰“ä¹±è·å–æ··åˆå¯¹
        perm = torch.randperm(batch_size)
        shuffled_data = positive_data[perm]
        
        # éšæœºæ··åˆæ¯”ä¾‹
        alpha = torch.rand(batch_size, 1, 1, 1) * \
                (self.alpha_range[1] - self.alpha_range[0]) + \
                self.alpha_range[0]
        
        negative_data = alpha * positive_data + (1 - alpha) * shuffled_data
        
        return negative_data
    
    @property
    def requires_labels(self):
        return False
```

---

## 7. ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’

### ç«‹å³è¡ŒåŠ¨ï¼ˆ1-2 å¤©ï¼‰
1. [ ] å…‹éš†ç°æœ‰çš„ FF å¼€æºå®ç°
2. [ ] è¿è¡ŒåŸºç¡€å®éªŒéªŒè¯ç¯å¢ƒ
3. [ ] åˆ›å»ºå®éªŒä»£ç æ¡†æ¶

### çŸ­æœŸï¼ˆ1 å‘¨ï¼‰
1. [ ] å®ç°æ‰€æœ‰ 10 ç§è´Ÿæ ·æœ¬ç­–ç•¥
2. [ ] åœ¨ MNIST ä¸Šè¿è¡Œåˆæ­¥å¯¹æ¯”
3. [ ] è®¾ç½®è‡ªåŠ¨åŒ–å®éªŒè„šæœ¬

### ä¸­æœŸï¼ˆ2-4 å‘¨ï¼‰
1. [ ] å®Œæˆä¸»è¦å¯¹æ¯”å®éªŒ
2. [ ] åˆ†æç»“æœï¼Œæ’°å†™åˆæ­¥æŠ¥å‘Š
3. [ ] è¯†åˆ«æœ€æœ‰æ½œåŠ›çš„ç­–ç•¥ç»„åˆ

### é•¿æœŸï¼ˆ1-2 æœˆï¼‰
1. [ ] æ·±å…¥åˆ†ææœ€ä½³ç­–ç•¥
2. [ ] æ’°å†™è®ºæ–‡/æŠ€æœ¯æŠ¥å‘Š
3. [ ] å¼€æºå®Œæ•´å®éªŒä»£ç 

---

## 8. å‚è€ƒæ–‡çŒ®

1. Hinton, G. (2022). The Forward-Forward Algorithm: Some Preliminary Investigations. arXiv:2212.13345
2. Gandhi et al. (2023). Extending the Forward Forward Algorithm. arXiv:2307.04205
3. Paliotta et al. (2023). Graph Neural Networks Go Forward-Forward. arXiv:2302.05282
4. Gat et al. (2023). Layer Collaboration in the Forward-Forward Algorithm. arXiv:2305.12393
5. Chen et al. (2024). Self-Contrastive Forward-Forward Algorithm. arXiv:2409.12184
6. Gong et al. (2025). Mono-Forward: Backpropagation-Free Algorithm. arXiv:2501.08756
7. Wu et al. (2024). Distance-Forward Learning. arXiv:2408.14577
8. Park et al. (2024). Forward Learning of Graph Neural Networks. arXiv:2403.11004

---

*æœ€åæ›´æ–°: 2026-02-04*
*è°ƒç ”è€…: Rios (FF-RQ2 Subagent)*
